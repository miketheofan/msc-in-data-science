{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfEpHS6hJr-B"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17023,
     "status": "ok",
     "timestamp": 1744698947523,
     "user": {
      "displayName": "Marios Mantzaris",
      "userId": "03416491895175165913"
     },
     "user_tz": -180
    },
    "id": "2nlFTejCJrr7",
    "outputId": "9f4f7244-7fae-4ece-c586-4e7efe455af9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnoQvXCdJwkx"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d92pT_pLJyrZ"
   },
   "outputs": [],
   "source": [
    "# PART 1: N-GRAM LANGUAGE MODEL IMPLEMENTATION\n",
    "# ===========================================\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, min_freq: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "\n",
    "        Args:\n",
    "            n: The size of n-grams (2 for bigram, 3 for trigram)\n",
    "            min_freq: Minimum frequency to include a word in vocabulary\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        # Main model components\n",
    "        self.vocabulary = set()  # Words in the vocabulary\n",
    "        self.word_counts = Counter()  # Counts of individual words\n",
    "        self.ngram_counts = defaultdict(Counter)  # Counts of n-grams\n",
    "        self.context_counts = defaultdict(int)  # Counts of (n-1)-grams (contexts)\n",
    "\n",
    "        # Model constants\n",
    "        self.UNK = \"<UNK>\"  # Out-of-vocabulary token\n",
    "        self.END = \"<end>\"  # End of sentence token\n",
    "\n",
    "        # Different start tokens for different n values\n",
    "        if n == 2:\n",
    "            self.START = [\"<start>\"]\n",
    "        elif n == 3:\n",
    "            self.START = [\"<start1>\", \"<start2>\"]\n",
    "        else:\n",
    "            self.START = [f\"<start{i}>\" for i in range(1, n)]\n",
    "\n",
    "        # Statistics\n",
    "        self.total_sentences = 0\n",
    "        self.total_tokens = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def preprocess_text(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Preprocess raw sentences into tokenized form.\n",
    "\n",
    "        Args:\n",
    "            sentences: List of raw text sentences\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences\n",
    "        \"\"\"\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Clean and tokenize the sentence\n",
    "            clean_sentence = sentence.lower().strip()\n",
    "            tokens = nltk.word_tokenize(clean_sentence)\n",
    "            tokenized_sentences.append(tokens)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def build_vocabulary(self, tokenized_sentences: List[List[str]]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from tokenized sentences based on minimum frequency.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            Set of vocabulary words\n",
    "        \"\"\"\n",
    "        # Count word occurrences\n",
    "        word_counter = Counter()\n",
    "        for sentence in tokenized_sentences:\n",
    "            word_counter.update(sentence)\n",
    "\n",
    "        # Create vocabulary with words that meet minimum frequency\n",
    "        vocabulary = {word for word, count in word_counter.items()\n",
    "                     if count >= self.min_freq}\n",
    "\n",
    "        # Always add special tokens to vocabulary\n",
    "        vocabulary.add(self.UNK)\n",
    "        vocabulary.add(self.END)\n",
    "        for token in self.START:\n",
    "            vocabulary.add(token)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def replace_oov_words(self, tokenized_sentences: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Replace out-of-vocabulary words with UNK token.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "\n",
    "        for sentence in tokenized_sentences:\n",
    "            processed_sentence = []\n",
    "            for token in sentence:\n",
    "                if token in self.vocabulary:\n",
    "                    processed_sentence.append(token)\n",
    "                else:\n",
    "                    processed_sentence.append(self.UNK)\n",
    "            processed_sentences.append(processed_sentence)\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "    def extract_ngrams(self, tokenized_sentences: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Extract n-grams from tokenized sentences and count their occurrences.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        for sentence in tokenized_sentences:\n",
    "            # Add start and end tokens\n",
    "            augmented_sentence = self.START + sentence + [self.END]\n",
    "            self.total_tokens += len(sentence) + 1  # +1 for END token\n",
    "\n",
    "            # Count individual words (unigrams)\n",
    "            self.word_counts.update(augmented_sentence)\n",
    "\n",
    "            # Extract and count n-grams\n",
    "            for i in range(len(augmented_sentence) - self.n + 1):\n",
    "                ngram = tuple(augmented_sentence[i:i + self.n])\n",
    "                prefix = ngram[:-1]  # Context (n-1 gram)\n",
    "                word = ngram[-1]     # Word being predicted\n",
    "\n",
    "                self.ngram_counts[prefix][word] += 1\n",
    "                self.context_counts[prefix] += 1\n",
    "\n",
    "    def train(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Train the n-gram language model on the provided corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of sentences\n",
    "        \"\"\"\n",
    "        self.total_sentences = len(corpus)\n",
    "        print(f\"Training {self.n}-gram model on {self.total_sentences} sentences...\")\n",
    "\n",
    "        # Preprocess the corpus\n",
    "        tokenized_sentences = self.preprocess_text(corpus)\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocabulary = self.build_vocabulary(tokenized_sentences)\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size: {self.vocabulary_size} words\")\n",
    "\n",
    "        # Replace OOV words\n",
    "        processed_sentences = self.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "        # Extract n-grams\n",
    "        self.extract_ngrams(processed_sentences)\n",
    "\n",
    "        print(f\"Extracted {sum(len(counts) for counts in self.ngram_counts.values())} unique {self.n}-grams\")\n",
    "        print(f\"Total tokens in corpus: {self.total_tokens}\")\n",
    "\n",
    "    def get_laplace_probability(self, word: str, context: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Laplace-smoothed probability P(word|context).\n",
    "\n",
    "        Args:\n",
    "            word: The word to calculate probability for\n",
    "            context: The preceding (n-1) words\n",
    "\n",
    "        Returns:\n",
    "            The conditional probability P(word|context)\n",
    "        \"\"\"\n",
    "        # Get counts with Laplace smoothing\n",
    "        count_ngram = self.ngram_counts[context][word]\n",
    "        count_context = self.context_counts[context]\n",
    "\n",
    "        # Apply Laplace smoothing (+1 to numerator, +V to denominator)\n",
    "        probability = (count_ngram + 1) / (count_context + self.vocabulary_size)\n",
    "\n",
    "        return probability\n",
    "\n",
    "    def get_log_probability(self, word: str, context: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log probability log(P(word|context)).\n",
    "\n",
    "        Args:\n",
    "            word: The word to calculate probability for\n",
    "            context: The preceding (n-1) words\n",
    "\n",
    "        Returns:\n",
    "            The log probability log(P(word|context))\n",
    "        \"\"\"\n",
    "        probability = self.get_laplace_probability(word, context)\n",
    "        return math.log2(probability)\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the log probability of a sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence: List of tokens in the sentence\n",
    "\n",
    "        Returns:\n",
    "            The log probability of the sentence\n",
    "        \"\"\"\n",
    "        # Replace OOV words with UNK\n",
    "        processed_sentence = [token if token in self.vocabulary else self.UNK for token in sentence]\n",
    "\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = self.START + processed_sentence + [self.END]\n",
    "\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # Calculate log probability for each word given its context\n",
    "        for i in range(len(self.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - self.n + 1:i])\n",
    "\n",
    "            log_prob += self.get_log_probability(word, context)\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6tEY9p9J2Vr"
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8ZGOfJ8XJ386"
   },
   "outputs": [],
   "source": [
    "# PART 2: CROSS-ENTROPY AND PERPLEXITY EVALUATION\n",
    "# ==============================================\n",
    "\n",
    "def calculate_cross_entropy(model: NGramLanguageModel, test_corpus: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy of a language model on a test corpus.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        test_corpus: List of test sentences\n",
    "\n",
    "    Returns:\n",
    "        Cross-entropy value\n",
    "    \"\"\"\n",
    "    # Preprocess test corpus\n",
    "    tokenized_sentences = model.preprocess_text(test_corpus)\n",
    "\n",
    "    # Replace OOV words\n",
    "    processed_sentences = model.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Calculate log probability for each sentence\n",
    "    for sentence in processed_sentences:\n",
    "        # We count end tokens but not start tokens in the total length\n",
    "        total_tokens += len(sentence) + 1  # +1 for END token\n",
    "\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = model.START + sentence + [model.END]\n",
    "\n",
    "        # Sum log probabilities for each word given its context\n",
    "        for i in range(len(model.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - model.n + 1:i])\n",
    "\n",
    "            # Get log probability\n",
    "            log_prob = model.get_log_probability(word, context)\n",
    "            total_log_prob += log_prob\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    cross_entropy = -total_log_prob / total_tokens\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "U9XFeldBJ4DD"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(cross_entropy: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy.\n",
    "\n",
    "    Args:\n",
    "        cross_entropy: Cross-entropy value\n",
    "\n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    return 2 ** cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjrSzAMJ7Ej"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S_iamUVrJ4Fh"
   },
   "outputs": [],
   "source": [
    "def generate_text(model: NGramLanguageModel,\n",
    "                 prompt: List[str],\n",
    "                 max_length: int = 20,\n",
    "                 method: str = \"greedy\",\n",
    "                 top_k: int = 5,\n",
    "                 temperature: float = 1.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text continuation based on the prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        max_length: Maximum length of the generated sequence\n",
    "        method: Generation method - \"greedy\", \"topk\", or \"nucleus\"\n",
    "        top_k: Number of top candidates to consider for sampling\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        List of words completing the prompt\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize with start tokens + prompt\n",
    "    generated_text = model.START + processed_prompt\n",
    "\n",
    "    # Generate text until we reach max_length or end token\n",
    "    for _ in range(max_length):\n",
    "        # Get the most recent (n-1) words as context\n",
    "        context = tuple(generated_text[-(model.n - 1):])\n",
    "\n",
    "        # Get next word based on the specified method\n",
    "        if method == \"greedy\":\n",
    "            next_word = get_next_word_greedy(model, context)\n",
    "        elif method == \"topk\":\n",
    "            next_word = get_next_word_topk(model, context, top_k, temperature)\n",
    "        elif method == \"nucleus\":\n",
    "            next_word = get_next_word_nucleus(model, context, p=0.9, temperature=temperature)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown generation method: {method}\")\n",
    "\n",
    "        # Add the generated word to the sequence\n",
    "        generated_text.append(next_word)\n",
    "\n",
    "        # Stop if we generated the end token\n",
    "        if next_word == model.END:\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated part (excluding start tokens and prompt)\n",
    "    return generated_text[len(model.START) + len(processed_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T_XE7epLJ4H0"
   },
   "outputs": [],
   "source": [
    "def get_next_word_greedy(model: NGramLanguageModel, context: tuple) -> str:\n",
    "    \"\"\"\n",
    "    Get the most probable next word given the context.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "\n",
    "    Returns:\n",
    "        Most probable next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Return the word with the highest probability\n",
    "    return max(candidates.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GPNhm9RaJ-8S"
   },
   "outputs": [],
   "source": [
    "def get_next_word_topk(model: NGramLanguageModel,\n",
    "                      context: tuple,\n",
    "                      k: int = 5,\n",
    "                      temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Sample next word from top-k most probable words.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "        k: Number of top candidates to consider\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Get top-k candidates\n",
    "    top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "    else:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    words = [word for word, _ in top_candidates]\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "b3gAhyEjJ-_C"
   },
   "outputs": [],
   "source": [
    "def get_next_word_nucleus(model: NGramLanguageModel,\n",
    "                         context: tuple,\n",
    "                         p: float = 0.9,\n",
    "                         temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) sampling for next word prediction.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "        p: Cumulative probability threshold\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Sort candidates by probability\n",
    "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = np.array([prob for _, prob in sorted_candidates])\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "    else:\n",
    "        probs = np.array([prob for _, prob in sorted_candidates])\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = np.cumsum(probs)\n",
    "\n",
    "    # Find smallest set of words with cumulative probability >= p\n",
    "    cutoff_idx = np.where(cumulative_probs >= p)[0][0] + 1\n",
    "\n",
    "    # Select only those candidates\n",
    "    top_p_candidates = sorted_candidates[:cutoff_idx]\n",
    "\n",
    "    # Re-normalize probabilities\n",
    "    top_p_probs = np.array([prob for _, prob in top_p_candidates])\n",
    "    top_p_probs = top_p_probs / np.sum(top_p_probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    words = [word for word, _ in top_p_candidates]\n",
    "    next_word = np.random.choice(words, p=top_p_probs)\n",
    "\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "s2cTq7csKA2b"
   },
   "outputs": [],
   "source": [
    "def beam_search(model: NGramLanguageModel,\n",
    "               prompt: List[str],\n",
    "               beam_width: int = 5,\n",
    "               max_length: int = 20) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Beam search for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        beam_width: Beam width\n",
    "        max_length: Maximum length of the generated sequence\n",
    "\n",
    "    Returns:\n",
    "        List of generated sequences (beams)\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize beams with start tokens + prompt\n",
    "    initial_sequence = model.START + processed_prompt\n",
    "    beams = [(initial_sequence, 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "    # Generate for max_length steps\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # Expand each beam\n",
    "        for sequence, score in beams:\n",
    "            # If the sequence ended, keep it as is\n",
    "            if sequence[-1] == model.END:\n",
    "                new_beams.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get context\n",
    "            context = tuple(sequence[-(model.n - 1):])\n",
    "\n",
    "            # Calculate probabilities for all possible next words\n",
    "            candidates = {}\n",
    "            for word in model.vocabulary:\n",
    "                # Skip UNK token for generation\n",
    "                if word == model.UNK:\n",
    "                    continue\n",
    "\n",
    "                log_prob = model.get_log_probability(word, context)\n",
    "                candidates[word] = log_prob\n",
    "\n",
    "            # Get top candidates\n",
    "            top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # Create new beams with expanded sequences\n",
    "            for word, log_prob in top_candidates:\n",
    "                new_sequence = sequence + [word]\n",
    "                new_score = score + log_prob\n",
    "                new_beams.append((new_sequence, new_score))\n",
    "\n",
    "        # Select top beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if all beams have ended\n",
    "        if all(sequence[-1] == model.END for sequence, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated parts (excluding start tokens and prompt)\n",
    "    start_len = len(model.START) + len(processed_prompt)\n",
    "    return [sequence[start_len:] for sequence, _ in beams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters corpus...\n",
      "Corpus split: 62999 train, 13500 validation, 13501 test sentences\n",
      "Training 2-gram model on 62999 sentences...\n",
      "Vocabulary size: 6273 words\n",
      "Extracted 204067 unique 2-grams\n",
      "Total tokens in corpus: 1156241\n",
      "['the', 'u', '.', '<end>']\n",
      "Training 3-gram model on 62999 sentences...\n",
      "Vocabulary size: 6274 words\n",
      "Extracted 530702 unique 3-grams\n",
      "Total tokens in corpus: 1156241\n",
      "['the', 'company', \"'\", 's']\n"
     ]
    }
   ],
   "source": [
    "def log_prob(p: float) -> float:\n",
    "    return math.log(p) if p > 0 else float('-inf')\n",
    "\n",
    "def context_aware_spelling_corrector(model: NGramLanguageModel,\n",
    "                                    noisy_sentence: List[str],\n",
    "                                    beam_width: int = 5, lambda_lm: float = 0.8, \n",
    "                                    lambda_err: float = 0.2) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Context-aware spelling correction using noisy channel model and beam search.\n",
    "\n",
    "    Args:\n",
    "        model: Trained n-gram language model\n",
    "        noisy_sentence: List of tokens with possible typos\n",
    "        beam_width: Beam width for beam search\n",
    "        lambda_lm: Weight for language model score\n",
    "        lambda_err: Weight for error model score\n",
    "\n",
    "    Returns:\n",
    "        Best corrected sentence as a list of tokens\n",
    "    \"\"\"\n",
    "    input_tokens = model.replace_oov_words(noisy_sentence)\n",
    "    # Initialize beams that holds (sequence_so_far, total_log_score)\n",
    "    beams = [(model.START, 0.0)]\n",
    "\n",
    "    for i, noisy_token in enumerate(input_tokens):\n",
    "        new_beams = []\n",
    "\n",
    "        for sequence, score in beams:\n",
    "            context = tuple(sequence[-(model.n - 1):])\n",
    "\n",
    "            for candidate in model.vocabulary:\n",
    "                if candidate == model.UNK:\n",
    "                    continue\n",
    "              \n",
    "                lm_logp = model.get_log_probability(candidate, context)\n",
    "\n",
    "                # Calculate the error model score (e.g., Levenshtein distance)\n",
    "                from nltk.metrics.distance import edit_distance\n",
    "                ld = edit_distance(noisy_token, candidate)\n",
    "                err_logp = log_prob(1 / (ld + 1))\n",
    "\n",
    "                total_score = score + lambda_lm * lm_logp + lambda_err * err_logp\n",
    "\n",
    "                new_sequence = sequence + [candidate]\n",
    "                new_beams.append((new_sequence, total_score))\n",
    "\n",
    "        import heapq\n",
    "        beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[1])\n",
    "\n",
    "    best_sequence = max(beams, key=lambda x: x[1])[0]\n",
    "    # Exclude start tokens\n",
    "    return best_sequence[len(model.START):]\n",
    "\n",
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='reuters', min_sentences=90000)\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10)\n",
    "trigram_model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'u', '.', '<end>']\n",
      "['the', 'company', \"'\", 's']\n"
     ]
    }
   ],
   "source": [
    "print(context_aware_spelling_corrector(bigram_model, [\"thsi\", \"is\", \"a\", \"test\"], beam_width=5))\n",
    "print(context_aware_spelling_corrector(trigram_model, [\"thsi\", \"is\", \"a\", \"test\"], beam_width=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHCBQPgXKFKd"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Gn8Ddtv2KB0j"
   },
   "outputs": [],
   "source": [
    "def load_and_split_corpus(corpus_name='reuters', min_sentences=100000):\n",
    "    \"\"\"\n",
    "    Load and split a corpus from NLTK into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        corpus_name: Name of the corpus to load\n",
    "        min_sentences: Minimum number of sentences to include\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_corpus, val_corpus, test_corpus)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {corpus_name} corpus...\")\n",
    "\n",
    "    if corpus_name == 'reuters':\n",
    "        from nltk.corpus import reuters\n",
    "        sentences = [\" \".join(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
    "\n",
    "        # Break into actual sentences\n",
    "        all_sentences = []\n",
    "        for text in sentences:\n",
    "            all_sentences.extend(nltk.sent_tokenize(text))\n",
    "\n",
    "    elif corpus_name == 'brown':\n",
    "        from nltk.corpus import brown\n",
    "        sentences = [\" \".join(brown.words(fileid)) for fileid in brown.fileids()]\n",
    "\n",
    "        # Break into actual sentences\n",
    "        all_sentences = []\n",
    "        for text in sentences:\n",
    "            all_sentences.extend(nltk.sent_tokenize(text))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown corpus: {corpus_name}\")\n",
    "\n",
    "    # Ensure we have enough sentences\n",
    "    if len(all_sentences) < min_sentences:\n",
    "        raise ValueError(f\"Corpus {corpus_name} has only {len(all_sentences)} sentences, \"\n",
    "                         f\"which is less than the required {min_sentences}.\")\n",
    "\n",
    "    # Shuffle sentences\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_sentences)\n",
    "\n",
    "    # Take a subset for faster processing if needed\n",
    "    sentences_subset = all_sentences[:min_sentences]\n",
    "\n",
    "    # Split into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_size = int(0.7 * len(sentences_subset))\n",
    "    val_size = int(0.15 * len(sentences_subset))\n",
    "\n",
    "    train_corpus = sentences_subset[:train_size]\n",
    "    val_corpus = sentences_subset[train_size:train_size + val_size]\n",
    "    test_corpus = sentences_subset[train_size + val_size:]\n",
    "\n",
    "    print(f\"Corpus split: {len(train_corpus)} train, {len(val_corpus)} validation, {len(test_corpus)} test sentences\")\n",
    "\n",
    "    return train_corpus, val_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33519,
     "status": "ok",
     "timestamp": 1744699158573,
     "user": {
      "displayName": "Marios Mantzaris",
      "userId": "03416491895175165913"
     },
     "user_tz": -180
    },
    "id": "QfpHHVESKDsL",
    "outputId": "fc8abfce-738f-4520-b577-a6380d171f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters corpus...\n",
      "Corpus split: 62999 train, 13500 validation, 13501 test sentences\n",
      "Training 2-gram model on 62999 sentences...\n",
      "Vocabulary size: 6273 words\n",
      "Extracted 204067 unique 2-grams\n",
      "Total tokens in corpus: 1156241\n",
      "Training 3-gram model on 62999 sentences...\n",
      "Vocabulary size: 6274 words\n",
      "Extracted 530702 unique 3-grams\n",
      "Total tokens in corpus: 1156241\n",
      "Common vocabulary size: 6272\n",
      "\n",
      "Evaluating on validation set...\n",
      "Bigram model - Cross-entropy: 7.8639, Perplexity: 232.9501\n",
      "Trigram model - Cross-entropy: 9.9089, Perplexity: 961.3537\n",
      "\n",
      "Evaluating on test set...\n",
      "Bigram model - Cross-entropy: 7.8611, Perplexity: 232.5016\n",
      "Trigram model - Cross-entropy: 9.9101, Perplexity: 962.1620\n",
      "\n",
      "Generating text completions:\n",
      "\n",
      "Bigram model completions:\n",
      "[Greedy] I would like to the company said .\n",
      "[Top-K] I would like to be a share , 000 vs loss of the company said it is expected to be a share .\n",
      "[Beam] I would like to the u .\n",
      "\n",
      "[Greedy] The president of the company said .\n",
      "[Top-K] The president of the company ' s .\n",
      "[Beam] The president of the u .\n",
      "\n",
      "[Greedy] According to recent years .\n",
      "[Top-K] According to recent rise in a year ' s .\n",
      "[Beam] According to recent years .\n",
      "\n",
      "[Greedy] In the last few months of the company said .\n",
      "[Top-K] In the last few weeks , '' said it will not to a share .\n",
      "[Beam] In the last few years .\n",
      "\n",
      "[Greedy] Experts say that the company said .\n",
      "[Top-K] Experts say that the bank of the company said .\n",
      "[Beam] Experts say that the u .\n",
      "\n",
      "\n",
      "Trigram model completions:\n",
      "[Greedy] I would like to see if the dollar , the company said .\n",
      "[Top-K] I would like to see the recent purchases .\n",
      "[Beam] I would like to buy the company said .\n",
      "\n",
      "[Greedy] The president of the company said .\n",
      "[Top-K] The president of the company said it is a subsidiary of the company said it would be made on a u .\n",
      "[Beam] The president of the u .\n",
      "\n",
      "[Greedy] According to recent severe returned returned returned returned returned returned returned returned returned returned returned returned returned returned returned returned returned returned returned\n",
      "[Top-K] According to recent agreements returned 919 conclude 919 conclude conclude 919 full full conclude 919 conclude conclude full returned full conclude 919 full\n",
      "[Beam] According to recent agreements on tariffs and trade , the u .\n",
      "\n",
      "[Greedy] In the last few weeks .\n",
      "[Top-K] In the last few days , the company said .\n",
      "[Beam] In the last few weeks .\n",
      "\n",
      "[Greedy] Experts say that the u .\n",
      "[Top-K] Experts say that the company said .\n",
      "[Beam] Experts say that the u .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run the full language modeling experiment.\n",
    "\"\"\"\n",
    "# Part 1\n",
    "# Load and split corpus\n",
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='reuters', min_sentences=90000)\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10)\n",
    "trigram_model.train(train_corpus)\n",
    "\n",
    "# Ensure both models use the same vocabulary\n",
    "common_vocab = bigram_model.vocabulary.intersection(trigram_model.vocabulary)\n",
    "bigram_model.vocabulary = common_vocab\n",
    "trigram_model.vocabulary = common_vocab\n",
    "bigram_model.vocabulary_size = len(common_vocab)\n",
    "trigram_model.vocabulary_size = len(common_vocab)\n",
    "\n",
    "print(f\"Common vocabulary size: {len(common_vocab)}\")\n",
    "\n",
    "# Part 2\n",
    "# Calculate cross-entropy and perplexity on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "bigram_ce_val = calculate_cross_entropy(bigram_model, val_corpus)\n",
    "bigram_ppl_val = calculate_perplexity(bigram_ce_val)\n",
    "\n",
    "trigram_ce_val = calculate_cross_entropy(trigram_model, val_corpus)\n",
    "trigram_ppl_val = calculate_perplexity(trigram_ce_val)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_val:.4f}, Perplexity: {bigram_ppl_val:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_val:.4f}, Perplexity: {trigram_ppl_val:.4f}\")\n",
    "\n",
    "# Calculate cross-entropy and perplexity on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "bigram_ce_test = calculate_cross_entropy(bigram_model, test_corpus)\n",
    "bigram_ppl_test = calculate_perplexity(bigram_ce_test)\n",
    "\n",
    "trigram_ce_test = calculate_cross_entropy(trigram_model, test_corpus)\n",
    "trigram_ppl_test = calculate_perplexity(trigram_ce_test)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_test:.4f}, Perplexity: {bigram_ppl_test:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_test:.4f}, Perplexity: {trigram_ppl_test:.4f}\")\n",
    "\n",
    "# Part 3\n",
    "# Generate text completions\n",
    "print(\"\\nGenerating text completions:\")\n",
    "\n",
    "prompts = [\n",
    "\"I would like to\",\n",
    "\"The president of\",\n",
    "\"According to recent\",\n",
    "\"In the last few\",\n",
    "\"Experts say that\"\n",
    "]\n",
    "\n",
    "print(\"\\nBigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(bigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != bigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(bigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != bigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(bigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != bigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()\n",
    "\n",
    "print(\"\\nTrigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(trigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != trigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(trigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != trigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(trigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != trigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

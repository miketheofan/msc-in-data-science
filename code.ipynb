{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfEpHS6hJr-B"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17023,
     "status": "ok",
     "timestamp": 1744698947523,
     "user": {
      "displayName": "Marios Mantzaris",
      "userId": "03416491895175165913"
     },
     "user_tz": -180
    },
    "id": "2nlFTejCJrr7",
    "outputId": "9f4f7244-7fae-4ece-c586-4e7efe455af9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Dict, Set, Callable\n",
    "from tqdm import tqdm\n",
    "from nltk.metrics.distance import edit_distance\n",
    "import heapq\n",
    "import string\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnoQvXCdJwkx"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "d92pT_pLJyrZ"
   },
   "outputs": [],
   "source": [
    "# PART 1: N-GRAM LANGUAGE MODEL IMPLEMENTATION\n",
    "# ===========================================\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, min_freq: int = 10):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "\n",
    "        Args:\n",
    "            n: The size of n-grams (2 for bigram, 3 for trigram)\n",
    "            min_freq: Minimum frequency to include a word in vocabulary\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "        # Main model components\n",
    "        self.vocabulary = set()  # Words in the vocabulary\n",
    "        self.word_counts = Counter()  # Counts of individual words\n",
    "        self.ngram_counts = defaultdict(Counter)  # Counts of n-grams\n",
    "        self.context_counts = defaultdict(int)  # Counts of (n-1)-grams (contexts)\n",
    "\n",
    "        # Model constants\n",
    "        self.UNK = \"<UNK>\"  # Out-of-vocabulary token\n",
    "        self.END = \"<end>\"  # End of sentence token\n",
    "\n",
    "        # Different start tokens for different n values\n",
    "        if n == 2:\n",
    "            self.START = [\"<start>\"]\n",
    "        elif n == 3:\n",
    "            self.START = [\"<start1>\", \"<start2>\"]\n",
    "        else:\n",
    "            self.START = [f\"<start{i}>\" for i in range(1, n)]\n",
    "\n",
    "        # Statistics\n",
    "        self.total_sentences = 0\n",
    "        self.total_tokens = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def preprocess_text(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Preprocess raw sentences into tokenized form.\n",
    "\n",
    "        Args:\n",
    "            sentences: List of raw text sentences\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences\n",
    "        \"\"\"\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            # Clean and tokenize the sentence\n",
    "            clean_sentence = sentence.lower().strip()\n",
    "            tokens = nltk.word_tokenize(clean_sentence)\n",
    "            tokenized_sentences.append(tokens)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def build_vocabulary(self, tokenized_sentences: List[List[str]]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from tokenized sentences based on minimum frequency.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            Set of vocabulary words\n",
    "        \"\"\"\n",
    "        # Count word occurrences\n",
    "        word_counter = Counter()\n",
    "        for sentence in tokenized_sentences:\n",
    "            word_counter.update(sentence)\n",
    "\n",
    "        # Create vocabulary with words that meet minimum frequency\n",
    "        vocabulary = {word for word, count in word_counter.items()\n",
    "                     if count >= self.min_freq}\n",
    "\n",
    "        # # Always add special tokens to vocabulary\n",
    "        vocabulary.add(self.UNK)\n",
    "        vocabulary.add(self.END)\n",
    "        for token in self.START:\n",
    "            vocabulary.add(token)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def replace_oov_words(self, tokenized_sentences: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Replace out-of-vocabulary words with UNK token.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "\n",
    "        for sentence in tokenized_sentences:\n",
    "            processed_sentence = []\n",
    "            for token in sentence:\n",
    "                if token in self.vocabulary:\n",
    "                    processed_sentence.append(token)\n",
    "                else:\n",
    "                    processed_sentence.append(self.UNK)\n",
    "            processed_sentences.append(processed_sentence)\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "    def extract_ngrams(self, tokenized_sentences: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Extract n-grams from tokenized sentences and count their occurrences.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        for sentence in tokenized_sentences:\n",
    "            # Add start and end tokens\n",
    "            augmented_sentence = self.START + sentence + [self.END]\n",
    "            self.total_tokens += len(sentence) + 1  # +1 for END token\n",
    "\n",
    "            # Count individual words (unigrams)\n",
    "            self.word_counts.update(augmented_sentence)\n",
    "\n",
    "            # Extract and count n-grams\n",
    "            for i in range(len(augmented_sentence) - self.n + 1):\n",
    "                ngram = tuple(augmented_sentence[i:i + self.n])\n",
    "                prefix = ngram[:-1]  # Context (n-1 gram)\n",
    "                word = ngram[-1]     # Word being predicted\n",
    "\n",
    "                self.ngram_counts[prefix][word] += 1\n",
    "                self.context_counts[prefix] += 1\n",
    "\n",
    "    def train(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Train the n-gram language model on the provided corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of sentences\n",
    "        \"\"\"\n",
    "        self.total_sentences = len(corpus)\n",
    "        print(f\"Training {self.n}-gram model on {self.total_sentences} sentences...\")\n",
    "\n",
    "        # Preprocess the corpus\n",
    "        tokenized_sentences = self.preprocess_text(corpus)\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocabulary = self.build_vocabulary(tokenized_sentences)\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size: {self.vocabulary_size} words\")\n",
    "\n",
    "        # Replace OOV words\n",
    "        processed_sentences = self.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "        # Extract n-grams\n",
    "        self.extract_ngrams(processed_sentences)\n",
    "\n",
    "        print(f\"Extracted {sum(len(counts) for counts in self.ngram_counts.values())} unique {self.n}-grams\")\n",
    "        print(f\"Total tokens in corpus: {self.total_tokens}\")\n",
    "\n",
    "    def get_laplace_probability(self, word: str, context: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Laplace-smoothed probability P(word|context).\n",
    "\n",
    "        Args:\n",
    "            word: The word to calculate probability for\n",
    "            context: The preceding (n-1) words\n",
    "\n",
    "        Returns:\n",
    "            The conditional probability P(word|context)\n",
    "        \"\"\"\n",
    "        # Get counts with Laplace smoothing\n",
    "        count_ngram = self.ngram_counts[context][word]\n",
    "        count_context = self.context_counts[context]\n",
    "\n",
    "        # Apply Laplace smoothing (+1 to numerator, +V to denominator)\n",
    "        probability = (count_ngram + 1) / (count_context + self.vocabulary_size)\n",
    "\n",
    "        return probability\n",
    "\n",
    "    def get_log_probability(self, word: str, context: tuple) -> float:\n",
    "        \"\"\"\n",
    "        Calculate log probability log(P(word|context)).\n",
    "\n",
    "        Args:\n",
    "            word: The word to calculate probability for\n",
    "            context: The preceding (n-1) words\n",
    "\n",
    "        Returns:\n",
    "            The log probability log(P(word|context))\n",
    "        \"\"\"\n",
    "        probability = self.get_laplace_probability(word, context)\n",
    "        return math.log2(probability)\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the log probability of a sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence: List of tokens in the sentence\n",
    "\n",
    "        Returns:\n",
    "            The log probability of the sentence\n",
    "        \"\"\"\n",
    "        # Replace OOV words with UNK\n",
    "        processed_sentence = [token if token in self.vocabulary else self.UNK for token in sentence]\n",
    "\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = self.START + processed_sentence + [self.END]\n",
    "\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # Calculate log probability for each word given its context\n",
    "        for i in range(len(self.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - self.n + 1:i])\n",
    "\n",
    "            log_prob += self.get_log_probability(word, context)\n",
    "\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6tEY9p9J2Vr"
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "8ZGOfJ8XJ386"
   },
   "outputs": [],
   "source": [
    "# PART 2: CROSS-ENTROPY AND PERPLEXITY EVALUATION\n",
    "# ==============================================\n",
    "\n",
    "def calculate_cross_entropy(model: NGramLanguageModel, test_corpus: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy of a language model on a test corpus.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        test_corpus: List of test sentences\n",
    "\n",
    "    Returns:\n",
    "        Cross-entropy value\n",
    "    \"\"\"\n",
    "    # Preprocess test corpus\n",
    "    tokenized_sentences = model.preprocess_text(test_corpus)\n",
    "\n",
    "    # Replace OOV words\n",
    "    processed_sentences = model.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Calculate log probability for each sentence\n",
    "    for sentence in processed_sentences:\n",
    "        # We count end tokens but not start tokens in the total length\n",
    "        total_tokens += len(sentence) + 1  # +1 for END token\n",
    "\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = model.START + sentence + [model.END]\n",
    "\n",
    "        # Sum log probabilities for each word given its context\n",
    "        for i in range(len(model.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - model.n + 1:i])\n",
    "\n",
    "            # Get log probability\n",
    "            log_prob = model.get_log_probability(word, context)\n",
    "            total_log_prob += log_prob\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    cross_entropy = -total_log_prob / total_tokens\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "U9XFeldBJ4DD"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(cross_entropy: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy.\n",
    "\n",
    "    Args:\n",
    "        cross_entropy: Cross-entropy value\n",
    "\n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    return 2 ** cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjrSzAMJ7Ej"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "S_iamUVrJ4Fh"
   },
   "outputs": [],
   "source": [
    "def generate_text(model: NGramLanguageModel,\n",
    "                 prompt: List[str],\n",
    "                 max_length: int = 20,\n",
    "                 method: str = \"greedy\",\n",
    "                 top_k: int = 5,\n",
    "                 temperature: float = 1.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text continuation based on the prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        max_length: Maximum length of the generated sequence\n",
    "        method: Generation method - \"greedy\", \"topk\", or \"nucleus\"\n",
    "        top_k: Number of top candidates to consider for sampling\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        List of words completing the prompt\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize with start tokens + prompt\n",
    "    generated_text = model.START + processed_prompt\n",
    "\n",
    "    # Generate text until we reach max_length or end token\n",
    "    for _ in range(max_length):\n",
    "        # Get the most recent (n-1) words as context\n",
    "        context = tuple(generated_text[-(model.n - 1):])\n",
    "\n",
    "        # Get next word based on the specified method\n",
    "        if method == \"greedy\":\n",
    "            next_word = get_next_word_greedy(model, context)\n",
    "        elif method == \"topk\":\n",
    "            next_word = get_next_word_topk(model, context, top_k, temperature)\n",
    "        elif method == \"nucleus\":\n",
    "            next_word = get_next_word_nucleus(model, context, p=0.9, temperature=temperature)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown generation method: {method}\")\n",
    "\n",
    "        # Add the generated word to the sequence\n",
    "        generated_text.append(next_word)\n",
    "\n",
    "        # Stop if we generated the end token\n",
    "        if next_word == model.END:\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated part (excluding start tokens and prompt)\n",
    "    return generated_text[len(model.START) + len(processed_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "T_XE7epLJ4H0"
   },
   "outputs": [],
   "source": [
    "def get_next_word_greedy(model: NGramLanguageModel, context: tuple) -> str:\n",
    "    \"\"\"\n",
    "    Get the most probable next word given the context.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "\n",
    "    Returns:\n",
    "        Most probable next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Return the word with the highest probability\n",
    "    return max(candidates.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "GPNhm9RaJ-8S"
   },
   "outputs": [],
   "source": [
    "def get_next_word_topk(model: NGramLanguageModel,\n",
    "                      context: tuple,\n",
    "                      k: int = 5,\n",
    "                      temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Sample next word from top-k most probable words.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "        k: Number of top candidates to consider\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Get top-k candidates\n",
    "    top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "    else:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    words = [word for word, _ in top_candidates]\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "b3gAhyEjJ-_C"
   },
   "outputs": [],
   "source": [
    "def get_next_word_nucleus(model: NGramLanguageModel,\n",
    "                         context: tuple,\n",
    "                         p: float = 0.9,\n",
    "                         temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Nucleus (top-p) sampling for next word prediction.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "        p: Cumulative probability threshold\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Sort candidates by probability\n",
    "    sorted_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = np.array([prob for _, prob in sorted_candidates])\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "    else:\n",
    "        probs = np.array([prob for _, prob in sorted_candidates])\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "    # Calculate cumulative probabilities\n",
    "    cumulative_probs = np.cumsum(probs)\n",
    "\n",
    "    # Find smallest set of words with cumulative probability >= p\n",
    "    cutoff_idx = np.where(cumulative_probs >= p)[0][0] + 1\n",
    "\n",
    "    # Select only those candidates\n",
    "    top_p_candidates = sorted_candidates[:cutoff_idx]\n",
    "\n",
    "    # Re-normalize probabilities\n",
    "    top_p_probs = np.array([prob for _, prob in top_p_candidates])\n",
    "    top_p_probs = top_p_probs / np.sum(top_p_probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    words = [word for word, _ in top_p_candidates]\n",
    "    next_word = np.random.choice(words, p=top_p_probs)\n",
    "\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "s2cTq7csKA2b"
   },
   "outputs": [],
   "source": [
    "def beam_search(model: NGramLanguageModel,\n",
    "               prompt: List[str],\n",
    "               beam_width: int = 5,\n",
    "               max_length: int = 20) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Beam search for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        beam_width: Beam width\n",
    "        max_length: Maximum length of the generated sequence\n",
    "\n",
    "    Returns:\n",
    "        List of generated sequences (beams)\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize beams with start tokens + prompt\n",
    "    initial_sequence = model.START + processed_prompt\n",
    "    beams = [(initial_sequence, 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "    # Generate for max_length steps\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # Expand each beam\n",
    "        for sequence, score in beams:\n",
    "            # If the sequence ended, keep it as is\n",
    "            if sequence[-1] == model.END:\n",
    "                new_beams.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get context\n",
    "            context = tuple(sequence[-(model.n - 1):])\n",
    "\n",
    "            # Calculate probabilities for all possible next words\n",
    "            candidates = {}\n",
    "            for word in model.vocabulary:\n",
    "                # Skip UNK token for generation\n",
    "                if word == model.UNK:\n",
    "                    continue\n",
    "\n",
    "                log_prob = model.get_log_probability(word, context)\n",
    "                candidates[word] = log_prob\n",
    "\n",
    "            # Get top candidates\n",
    "            top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # Create new beams with expanded sequences\n",
    "            for word, log_prob in top_candidates:\n",
    "                new_sequence = sequence + [word]\n",
    "                new_score = score + log_prob\n",
    "                new_beams.append((new_sequence, new_score))\n",
    "\n",
    "        # Select top beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if all beams have ended\n",
    "        if all(sequence[-1] == model.END for sequence, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated parts (excluding start tokens and prompt)\n",
    "    start_len = len(model.START) + len(processed_prompt)\n",
    "    return [sequence[start_len:] for sequence, _ in beams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(p: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute log-probability with safe handling for zero.\n",
    "\n",
    "    Args:\n",
    "        p: A probability value\n",
    "\n",
    "    Returns:\n",
    "        Natural log of p, or -inf if p is 0\n",
    "    \"\"\"\n",
    "    return math.log(p) if p > 0 else float('-inf')\n",
    "\n",
    "\n",
    "def softmax(scores):\n",
    "    \"\"\"\n",
    "    Compute softmax over a list of scores.\n",
    "\n",
    "    Args:\n",
    "        scores: List of float scores\n",
    "\n",
    "    Returns:\n",
    "        List of normalized probabilities\n",
    "    \"\"\"\n",
    "    exp_scores = [math.exp(s - max(scores)) for s in scores]\n",
    "    total = sum(exp_scores)\n",
    "    return [e / total for e in exp_scores]\n",
    "\n",
    "\n",
    "def calculate_lm_score(candidate: str, context: Tuple[str, ...], model: NGramLanguageModel) -> float:\n",
    "    \"\"\"\n",
    "    Get the language model log-probability of a candidate given context.\n",
    "\n",
    "    Args:\n",
    "        candidate: Word to score\n",
    "        context: Tuple of previous words\n",
    "        model: Trained N-gram language model\n",
    "\n",
    "    Returns:\n",
    "        Log-probability of candidate given context\n",
    "    \"\"\"\n",
    "    return model.get_log_probability(candidate, context)\n",
    "\n",
    "\n",
    "def calculate_error_score(noisy_token: str, candidate: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the log-probability of a candidate based on its edit distance.\n",
    "\n",
    "    Args:\n",
    "        noisy_token: Original word\n",
    "        candidate: Possible correction\n",
    "\n",
    "    Returns:\n",
    "        Log-probability based on inverse edit distance\n",
    "    \"\"\"\n",
    "    edit_dist = nltk.edit_distance(noisy_token, candidate)\n",
    "    return log_prob(1 / (edit_dist + 1))\n",
    "\n",
    "def combine_scores(lm_score: float, error_score: float, lambda_lm: float = 0.8, lambda_err: float = 0.2) -> float:\n",
    "    \"\"\"\n",
    "    Combine language model and error model scores using weighted sum.\n",
    "\n",
    "    Args:\n",
    "        lm_score: Log-probability from language model\n",
    "        error_score: Log-probability from error model\n",
    "        lambda_lm: Weight for language model\n",
    "        lambda_err: Weight for error model\n",
    "\n",
    "    Returns:\n",
    "        Combined score\n",
    "    \"\"\"\n",
    "    return lambda_lm * lm_score + lambda_err * error_score\n",
    "\n",
    "def generate_candidates(\n",
    "    noisy_token: str,\n",
    "    vocabulary: Set[str],\n",
    "    max_edit_distance: int = 2,\n",
    "    skip_oov: bool = True\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate candidate corrections within a max edit distance.\n",
    "\n",
    "    Args:\n",
    "        noisy_token: Token to correct\n",
    "        vocabulary: Set of known words\n",
    "        max_edit_distance: Max allowed edit distance\n",
    "        skip_oov: If True, return token if itâ€™s in vocabulary\n",
    "\n",
    "    Returns:\n",
    "        List of candidate words\n",
    "    \"\"\"\n",
    "    if skip_oov and noisy_token in vocabulary:\n",
    "        return [noisy_token]\n",
    "\n",
    "    candidates = []\n",
    "    for word in vocabulary:\n",
    "        if word == \"<UNK>\":\n",
    "            continue\n",
    "        if nltk.edit_distance(noisy_token, word) <= max_edit_distance:\n",
    "            candidates.append(word)\n",
    "    return candidates or [noisy_token]\n",
    "\n",
    "def beam_search_step(\n",
    "    beams: List[Tuple[List[str], float]],\n",
    "    noisy_token: str,\n",
    "    model: NGramLanguageModel,\n",
    "    vocabulary: Set[str],\n",
    "    beam_width: int = 5,\n",
    "    lambda_lm: float = 0.8,\n",
    "    lambda_err: float = 0.2,\n",
    "    max_edit_distance: int = 2,\n",
    "    skip_oov: bool = True\n",
    ") -> List[Tuple[List[str], float]]:\n",
    "    \"\"\"\n",
    "    Expand beam sequences with possible corrections for the next token.\n",
    "\n",
    "    Args:\n",
    "        beams: List of (sequence, score) pairs\n",
    "        noisy_token: Token to correct\n",
    "        model: N-gram language model\n",
    "        vocabulary: Set of valid words\n",
    "        beam_width: Max beams to keep\n",
    "        lambda_lm: LM score weight\n",
    "        lambda_err: Error score weight\n",
    "        max_edit_distance: Edit distance threshold\n",
    "        skip_oov: If True, use original token if in vocab\n",
    "\n",
    "    Returns:\n",
    "        Updated list of top-k beams\n",
    "    \"\"\"\n",
    "    new_beams = []\n",
    "    # Collect for each candidate\": lm_score, err_score, total_score\n",
    "    candidate_info = [] \n",
    "\n",
    "    for sequence, score in beams:\n",
    "        context = tuple(sequence[-(model.n - 1):])\n",
    "        candidates = generate_candidates(noisy_token, vocabulary, max_edit_distance, skip_oov)\n",
    "\n",
    "        for candidate in candidates:\n",
    "            lm_score = calculate_lm_score(candidate, context, model)\n",
    "            err_score = calculate_error_score(noisy_token, candidate)\n",
    "            total_score = combine_scores(lm_score, err_score, lambda_lm, lambda_err)\n",
    "\n",
    "            candidate_info.append((candidate, lm_score, err_score, total_score))\n",
    "\n",
    "            new_sequence = sequence + [candidate]\n",
    "            new_score = score + total_score\n",
    "            new_beams.append((new_sequence, new_score))\n",
    "\n",
    "    if not new_beams:\n",
    "        print(f\"Warning: No valid candidates for '{noisy_token}'. Using fallback.\")\n",
    "        for sequence, score in beams:\n",
    "            new_sequence = sequence + [noisy_token]\n",
    "        new_beams.append((new_sequence, score))\n",
    "\n",
    "    top_beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[1])\n",
    "    top_tokens = [beam[0][-1] for beam in top_beams]\n",
    "\n",
    "    print(f\"\\nToken: '{noisy_token}' | Context: {context}\")\n",
    "    print(\"Top candidates:\")\n",
    "    print(f\"{'Candidate':<15} {'LM Score':>10} {'Error Score':>12} {'Combined':>12}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for token in top_tokens:\n",
    "        for cand, lm, err, combined in candidate_info:\n",
    "            if cand == token:\n",
    "                print(f\"{cand:<15} {lm:>+10.4f} {err:>+12.4f} {combined:>+12.4f}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Best candidate selected: {top_tokens[0]}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    return top_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_aware_spelling_corrector(\n",
    "    model,\n",
    "    noisy_sentence: List[str],\n",
    "    beam_width: int = 5,\n",
    "    lambda_lm: float = 0.8,\n",
    "    lambda_err: float = 0.2,\n",
    "    max_edit_distance: int = 2,\n",
    "    skip_oov: bool = True,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform context-aware spelling correction on a noisy sentence using beam search.\n",
    "\n",
    "    Args:\n",
    "        model: Trained N-gram language model with .START and .vocabulary\n",
    "        noisy_sentence: List of potentially misspelled tokens\n",
    "        beam_width: Number of sequences to keep per step\n",
    "        lambda_lm: Weight for the language model score\n",
    "        lambda_err: Weight for the error model score\n",
    "        max_edit_distance: Max edit distance for generating candidates\n",
    "        skip_oov: If True, preserve in-vocabulary words\n",
    "\n",
    "    Returns:\n",
    "        A list of corrected tokens\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting correction for sentence: {' '.join(noisy_sentence)}\\n\")\n",
    "    beams = [(model.START, 0.0)]\n",
    "\n",
    "    for noisy_token in noisy_sentence:\n",
    "        if skip_oov and noisy_token in model.vocabulary:\n",
    "            print(f\"\\nToken: '{noisy_token}' (in vocabulary, skipping correction)\")\n",
    "            # print(f\"{'Candidate':<15} {'LM Score':>10} {'Error Score':>12} {'Combined':>12}\")\n",
    "            # print(\"-\" * 55)\n",
    "\n",
    "            # # Treat it as a \"perfect candidate\" with scores 0.0 (no penalty)\n",
    "            # for sequence, score in beams:\n",
    "            #     lm_score = calculate_lm_score(noisy_token, tuple(sequence[-(model.n - 1):]), model)\n",
    "            #     err_score = 0.0  # No error\n",
    "            #     combined_score = combine_scores(lm_score, err_score, lambda_lm, lambda_err)\n",
    "\n",
    "            #     print(f\"{noisy_token:<15} {lm_score:>+10.4f} {err_score:>+12.4f} {combined_score:>+12.4f}\")\n",
    "\n",
    "            # print(f\"Best candidate selected: {noisy_token}\")\n",
    "            # print(\"-\" * 55)\n",
    "\n",
    "            # Update beams: just append the noisy_token without penalty\n",
    "            new_beams = []\n",
    "            for sequence, score in beams:\n",
    "                new_sequence = sequence + [noisy_token]\n",
    "                new_beams.append((new_sequence, score))\n",
    "            beams = new_beams\n",
    "            continue\n",
    "\n",
    "        # Otherwise, expand beams normally\n",
    "        beams = beam_search_step(\n",
    "            beams, noisy_token, model, model.vocabulary,\n",
    "            beam_width, lambda_lm, lambda_err, max_edit_distance, skip_oov\n",
    "        )\n",
    "\n",
    "    best_sequence = max(beams, key=lambda x: x[1])[0]\n",
    "    corrected = best_sequence[len(model.START):]\n",
    "    print(f\"Final corrected sentence: {' '.join(corrected)}\")\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading brown corpus...\n",
      "Corpus split: 39621 train, 8490 validation, 8491 test sentences\n",
      "Training 2-gram model on 39621 sentences...\n",
      "Vocabulary size: 6637 words\n",
      "Extracted 209440 unique 2-grams\n",
      "Total tokens in corpus: 859931\n",
      "Training 3-gram model on 39621 sentences...\n",
      "Vocabulary size: 6638 words\n",
      "Extracted 519364 unique 3-grams\n",
      "Total tokens in corpus: 859931\n"
     ]
    }
   ],
   "source": [
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='brown')\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10)\n",
    "trigram_model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting correction for sentence: let us sey we are freends\n",
      "\n",
      "\n",
      "Token: 'let' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'us' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sey' | Context: ('us',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "by                -10.2072      -1.0986      -6.5638\n",
      "say               -10.7922      -0.6931      -6.7526\n",
      "see               -11.2072      -0.6931      -7.0016\n",
      "so                -11.7922      -1.0986      -7.5148\n",
      "they              -11.7922      -1.0986      -7.5148\n",
      "Best candidate selected: by\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'we' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'are' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'freends' | Context: ('are',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "freed             -12.2524      -1.0986      -7.7909\n",
      "freed             -12.2524      -1.0986      -7.7909\n",
      "freed             -12.2524      -1.0986      -7.7909\n",
      "friends           -13.2524      -0.6931      -8.2287\n",
      "friend            -13.2524      -1.0986      -8.3909\n",
      "Best candidate selected: freed\n",
      "-------------------------------------------------------\n",
      "Final corrected sentence: let us by we are freed\n",
      "\n",
      "\n",
      "Starting correction for sentence: in consequencaae of her sistero 's marriange , been moistress of hois house from a vry early period\n",
      "\n",
      "\n",
      "Token: 'in' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'consequencaae' | Context: ('in',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "consequence       -12.8067      -1.0986      -8.1235\n",
      "Best candidate selected: consequence\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'her' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sistero' | Context: ('her',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "sister            -12.0970      -0.6931      -7.5355\n",
      "sitter            -13.0970      -1.0986      -8.2977\n",
      "Best candidate selected: sister\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ''s' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'marriange' | Context: (\"'s\",)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "marriage          -13.3807      -0.6931      -8.3057\n",
      "marriages         -13.3807      -1.0986      -8.4679\n",
      "marriage          -13.3807      -0.6931      -8.3057\n",
      "marriages         -13.3807      -1.0986      -8.4679\n",
      "Best candidate selected: marriage\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'been' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'moistress' | Context: ('been',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "moistress         -13.0277      +0.0000      -7.8166\n",
      "moistress         -13.0277      +0.0000      -7.8166\n",
      "moistress         -13.0277      +0.0000      -7.8166\n",
      "moistress         -13.0277      +0.0000      -7.8166\n",
      "Best candidate selected: moistress\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'hois' | Context: ('of',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "his                -5.7650      -0.6931      -3.7362\n",
      "his                -5.7650      -0.6931      -3.7362\n",
      "this               -6.3934      -1.0986      -4.2755\n",
      "this               -6.3934      -1.0986      -4.2755\n",
      "his                -5.7650      -0.6931      -3.7362\n",
      "Best candidate selected: his\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'house' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'from' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'vry' | Context: ('a',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "very               -7.9273      -0.6931      -5.0337\n",
      "very               -7.9273      -0.6931      -5.0337\n",
      "very               -7.9273      -0.6931      -5.0337\n",
      "day                -8.7540      -1.0986      -5.6918\n",
      "very               -7.9273      -0.6931      -5.0337\n",
      "Best candidate selected: very\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'early' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'period' (in vocabulary, skipping correction)\n",
      "Final corrected sentence: in consequence of her sister 's marriage , been moistress of his house from a very early period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"let us sey we are freends\",\n",
    "    \"in consequencaae of her sistero's marriange, been moistress of hois house from a vry early period\",\n",
    "    # \"Tomorrrow well bring somethiing new, so leav today as a memoory.\",\n",
    "    # \"He wento too the storr to by some bred and mlik.\",\n",
    "    # \"Ths is an exampel of a sentense with severl erors.\",\n",
    "    # \"Wee shuld definately do ths agan some day.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected = context_aware_spelling_corrector(\n",
    "        bigram_model,\n",
    "        nltk.word_tokenize(sentence),\n",
    "        beam_width=5,\n",
    "        lambda_lm=0.6,\n",
    "        lambda_err=0.4,\n",
    "        skip_oov=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting correction for sentence: let us sey we are freends\n",
      "\n",
      "\n",
      "Token: 'let' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'us' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sey' | Context: ('let', 'us')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "see               -11.1207      -0.6931      -6.9497\n",
      "say               -11.1207      -0.6931      -6.9497\n",
      "sky               -12.7056      -0.6931      -7.9006\n",
      "set               -12.7056      -0.6931      -7.9006\n",
      "sex               -12.7056      -0.6931      -7.9006\n",
      "Best candidate selected: see\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'we' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'are' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'freends' | Context: ('we', 'are')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "friends           -12.7296      -0.6931      -7.9150\n",
      "friends           -12.7296      -0.6931      -7.9150\n",
      "friend            -12.7296      -1.0986      -8.0772\n",
      "trends            -12.7296      -1.0986      -8.0772\n",
      "freed             -12.7296      -1.0986      -8.0772\n",
      "Best candidate selected: friends\n",
      "-------------------------------------------------------\n",
      "Final corrected sentence: let us see we are friends\n",
      "\n",
      "\n",
      "Starting correction for sentence: in consequencaae of her sistero 's marriange , been moistress of hois house from a vry early period\n",
      "\n",
      "\n",
      "Token: 'in' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'consequencaae' | Context: ('<start2>', 'in')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "consequence       -12.9370      -1.0986      -8.2016\n",
      "Best candidate selected: consequence\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'her' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sistero' | Context: ('of', 'her')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "sister            -12.7322      -0.6931      -7.9166\n",
      "sitter            -12.7322      -1.0986      -8.0787\n",
      "Best candidate selected: sister\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ''s' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'marriange' | Context: ('sitter', \"'s\")\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "marriage          -12.6970      -0.6931      -7.8954\n",
      "marriage          -12.6970      -0.6931      -7.8954\n",
      "marriages         -12.6970      -1.0986      -8.0576\n",
      "marriages         -12.6970      -1.0986      -8.0576\n",
      "Best candidate selected: marriage\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'been' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'moistress' | Context: (',', 'been')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "moistress         -12.6983      +0.0000      -7.6190\n",
      "moistress         -12.6983      +0.0000      -7.6190\n",
      "moistress         -12.6983      +0.0000      -7.6190\n",
      "moistress         -12.6983      +0.0000      -7.6190\n",
      "Best candidate selected: moistress\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'hois' | Context: ('moistress', 'of')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "his               -12.6965      -0.6931      -7.8952\n",
      "his               -12.6965      -0.6931      -7.8952\n",
      "does              -12.6965      -1.0986      -8.0574\n",
      "noise             -12.6965      -1.0986      -8.0574\n",
      "home              -12.6965      -1.0986      -8.0574\n",
      "Best candidate selected: his\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'house' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'from' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'vry' | Context: ('from', 'a')\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "very              -11.7337      -0.6931      -7.3174\n",
      "very              -11.7337      -0.6931      -7.3174\n",
      "very              -11.7337      -0.6931      -7.3174\n",
      "very              -11.7337      -0.6931      -7.3174\n",
      "very              -11.7337      -0.6931      -7.3174\n",
      "Best candidate selected: very\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'early' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'period' (in vocabulary, skipping correction)\n",
      "Final corrected sentence: in consequence of her sister 's marriage , been moistress of his house from a very early period\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"let us sey we are freends\",\n",
    "    \"in consequencaae of her sistero's marriange, been moistress of hois house from a vry early period\",\n",
    "    # \"Tomorrrow well bring somethiing new, so leav today as a memoory.\",\n",
    "    # \"He wento too the storr to by some bred and mlik.\",\n",
    "    # \"Ths is an exampel of a sentense with severl erors.\",\n",
    "    # \"Wee shuld definately do ths agan some day.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected = context_aware_spelling_corrector(\n",
    "        trigram_model,\n",
    "        nltk.word_tokenize(sentence),\n",
    "        beam_width=5,\n",
    "        lambda_lm=0.6,\n",
    "        lambda_err=0.4,\n",
    "        skip_oov=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Artificial Test Dataset (class + usage example)\n",
    "# ------------------------------------------------------------------\n",
    "# Before running this cell, make sure you have already defined `test_corpus` \n",
    "# (e.g., via:\n",
    "#    train_corpus, val_corpus, test_corpus = load_and_split_corpus(\n",
    "#        corpus_name='reuters', min_sentences=90000)\n",
    "# ) so that `test_corpus` is a List[str] of clean sentences.\n",
    "class ArtificialTestDataset:\n",
    "    def __init__(self, sentences, error_prob=0.05, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the artificial test dataset generator.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): List of clean sentences to corrupt.\n",
    "            error_prob (float): Probability of replacing each non-space character.\n",
    "            seed (int, optional): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.error_prob = error_prob\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        # Character set for random replacements (excluding space)\n",
    "        self.chars = list(string.ascii_letters + string.digits + string.punctuation)\n",
    "\n",
    "    def _corrupt_char(self, c):\n",
    "        # Do not corrupt whitespace; apply corruption with given probability\n",
    "        if c.isspace() or random.random() > self.error_prob:\n",
    "            return c\n",
    "        # Choose a random replacement different from the original\n",
    "        replacement = random.choice(self.chars)\n",
    "        while replacement == c:\n",
    "            replacement = random.choice(self.chars)\n",
    "        return replacement\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate the corrupted dataset.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Corrupted sentences.\n",
    "        \"\"\"\n",
    "        corrupted = []\n",
    "        for sentence in self.sentences:\n",
    "            corrupted_sentence = ''.join(self._corrupt_char(c) for c in sentence)\n",
    "            corrupted.append(corrupted_sentence)\n",
    "        return corrupted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing original vs. corrupted for first 5 sentences:\n",
      "============================================================\n",
      "Original:  O beautiful for patriot dream that sees beyond the years thine alabaster cities gleam undimmed by human tears .\n",
      "Corrupted: O Jeautifll fzr patriot dream that 6ees beyond Dhe years thine alabaster cities gleam undimmed by human tears .\n",
      "------------------------------------------------------------\n",
      "Original:  ( cf.\n",
      "Corrupted: ( cf.\n",
      "------------------------------------------------------------\n",
      "Original:  The Village office of Western Union with George Towsley as manager and telegrapher continued in Hard's drugstore until 1905 .\n",
      "Corrupted: The Villag} office of WesterP Union with George Towsley as manager )nd telegrapher continued in Hard's drugstore until 1905 .\n",
      "------------------------------------------------------------\n",
      "Original:  As if this was a signal , Poet abruptly began to thrash the water and the quick movement slowly made them sink through the water .\n",
      "Corrupted: As if this was a signal , Poet abruptly began to thrash the water and the quic) movement slowly made tqem sinj thrVugh the water .\n",
      "------------------------------------------------------------\n",
      "Original:  Routine determinations were made for dissolved oxygen in the mixed liquor and for oxygen uptake rates .\n",
      "Corrupted: Routine deQerminati8ns were yade for disHolved oxygen in 'he mixed liquor and for oxygen uptake rates .\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now, with test_corpus loaded, generate the corrupted dataset and display samples:\n",
    "generator = ArtificialTestDataset(test_corpus, error_prob=0.05, seed=42)\n",
    "corrupted_test_corpus = generator.generate()\n",
    "\n",
    "print(\"Showing original vs. corrupted for first 5 sentences:\")\n",
    "print(\"=\" * 60)\n",
    "for orig, corrupt in zip(test_corpus[:5], corrupted_test_corpus[:5]):\n",
    "    print(f\"Original:  {orig}\")\n",
    "    print(f\"Corrupted: {corrupt}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHCBQPgXKFKd"
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "Gn8Ddtv2KB0j"
   },
   "outputs": [],
   "source": [
    "def load_and_split_corpus(corpus_name='reuters', min_sentences=-1):\n",
    "    \"\"\"\n",
    "    Load and split a corpus from NLTK into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        corpus_name: Name of the corpus to load\n",
    "        min_sentences: Minimum number of sentences to include\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_corpus, val_corpus, test_corpus)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {corpus_name} corpus...\")\n",
    "\n",
    "    if corpus_name == 'reuters':\n",
    "        from nltk.corpus import reuters\n",
    "        sentences = [\" \".join(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
    "\n",
    "    elif corpus_name == 'brown':\n",
    "        from nltk.corpus import brown\n",
    "        sentences = [\" \".join(brown.words(fileid)) for fileid in brown.fileids()]\n",
    "\n",
    "    elif corpus_name == 'gutenberg':\n",
    "        from nltk.corpus import gutenberg\n",
    "        sentences = [\" \".join(gutenberg.words(fileid)) for fileid in gutenberg.fileids()]\n",
    "\n",
    "    elif corpus_name == 'all':\n",
    "        from nltk.corpus import reuters, brown, gutenberg\n",
    "        sentences = []\n",
    "\n",
    "        # Combine texts from all three corpora\n",
    "        sentences += [\" \".join(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
    "        sentences += [\" \".join(brown.words(fileid)) for fileid in brown.fileids()]\n",
    "        sentences += [\" \".join(gutenberg.words(fileid)) for fileid in gutenberg.fileids()]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown corpus: {corpus_name}\")\n",
    "\n",
    "    # Break into actual sentences\n",
    "    all_sentences = []\n",
    "    for text in sentences:\n",
    "        all_sentences.extend(nltk.sent_tokenize(text))\n",
    "\n",
    "\n",
    "    # Ensure we have enough sentences\n",
    "    if len(all_sentences) < min_sentences:\n",
    "        raise ValueError(f\"Corpus {corpus_name} has only {len(all_sentences)} sentences, \"\n",
    "                         f\"which is less than the required {min_sentences}.\")\n",
    "\n",
    "    # Shuffle sentences\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_sentences)\n",
    "\n",
    "    # Take a subset for faster processing if needed\n",
    "    sentences_subset = all_sentences[:min_sentences]\n",
    "\n",
    "    # Split into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_size = int(0.7 * len(sentences_subset))\n",
    "    val_size = int(0.15 * len(sentences_subset))\n",
    "\n",
    "    train_corpus = sentences_subset[:train_size]\n",
    "    val_corpus = sentences_subset[train_size:train_size + val_size]\n",
    "    test_corpus = sentences_subset[train_size + val_size:]\n",
    "\n",
    "    print(f\"Corpus split: {len(train_corpus)} train, {len(val_corpus)} validation, {len(test_corpus)} test sentences\")\n",
    "\n",
    "    return train_corpus, val_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33519,
     "status": "ok",
     "timestamp": 1744699158573,
     "user": {
      "displayName": "Marios Mantzaris",
      "userId": "03416491895175165913"
     },
     "user_tz": -180
    },
    "id": "QfpHHVESKDsL",
    "outputId": "fc8abfce-738f-4520-b577-a6380d171f67"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the full language modeling experiment.\n",
    "\"\"\"\n",
    "# Part 1\n",
    "# Load and split corpus\n",
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='reuters', min_sentences=90000)\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10)\n",
    "trigram_model.train(train_corpus)\n",
    "\n",
    "# Ensure both models use the same vocabulary\n",
    "common_vocab = bigram_model.vocabulary.intersection(trigram_model.vocabulary)\n",
    "bigram_model.vocabulary = common_vocab\n",
    "trigram_model.vocabulary = common_vocab\n",
    "bigram_model.vocabulary_size = len(common_vocab)\n",
    "trigram_model.vocabulary_size = len(common_vocab)\n",
    "\n",
    "print(f\"Common vocabulary size: {len(common_vocab)}\")\n",
    "\n",
    "# Part 2\n",
    "# Calculate cross-entropy and perplexity on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "bigram_ce_val = calculate_cross_entropy(bigram_model, val_corpus)\n",
    "bigram_ppl_val = calculate_perplexity(bigram_ce_val)\n",
    "\n",
    "trigram_ce_val = calculate_cross_entropy(trigram_model, val_corpus)\n",
    "trigram_ppl_val = calculate_perplexity(trigram_ce_val)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_val:.4f}, Perplexity: {bigram_ppl_val:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_val:.4f}, Perplexity: {trigram_ppl_val:.4f}\")\n",
    "\n",
    "# Calculate cross-entropy and perplexity on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "bigram_ce_test = calculate_cross_entropy(bigram_model, test_corpus)\n",
    "bigram_ppl_test = calculate_perplexity(bigram_ce_test)\n",
    "\n",
    "trigram_ce_test = calculate_cross_entropy(trigram_model, test_corpus)\n",
    "trigram_ppl_test = calculate_perplexity(trigram_ce_test)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_test:.4f}, Perplexity: {bigram_ppl_test:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_test:.4f}, Perplexity: {trigram_ppl_test:.4f}\")\n",
    "\n",
    "# Part 3\n",
    "# Generate text completions\n",
    "print(\"\\nGenerating text completions:\")\n",
    "\n",
    "prompts = [\n",
    "\"I would like to\",\n",
    "\"The president of\",\n",
    "\"According to recent\",\n",
    "\"In the last few\",\n",
    "\"Experts say that\"\n",
    "]\n",
    "\n",
    "print(\"\\nBigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(bigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != bigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(bigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != bigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(bigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != bigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()\n",
    "\n",
    "print(\"\\nTrigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(trigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != trigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(trigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != trigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(trigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != trigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

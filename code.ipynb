{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets\n",
    "%pip install -U nltk\n",
    "%pip install transformers tiktoken\n",
    "\n",
    "%nltk.download(\"punkt\")\n",
    "%nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from collections import Counter\n",
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set #tokens: 2051910\n",
      "Dev set #tokens: 213886\n",
      "Test set #tokens: 241211\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Convert splits into plain text strings\n",
    "corpus = {\n",
    "    \"train\": \" \".join(raw_dataset[\"train\"][\"text\"]),\n",
    "    \"dev\": \" \".join(raw_dataset[\"validation\"][\"text\"]),\n",
    "    \"test\":  \" \".join(raw_dataset[\"test\"][\"text\"]),\n",
    "}\n",
    "\n",
    "# Display token counts\n",
    "for split in corpus:\n",
    "    print(f\"{split.capitalize()} set #tokens: {len(corpus[split].split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sents(sents: list, n_first:int) -> None:\n",
    "  for sent in sents[:n_first]:\n",
    "    print(sent)\n",
    "    print(\"___________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  = Valkyria Chronicles III = \n",
      "   Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit .\n",
      "___________________\n",
      "Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable .\n",
      "___________________\n",
      "Released in January 2011 in Japan , it is the third game in the Valkyria series .\n",
      "___________________\n",
      "Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .\n",
      "___________________\n",
      "The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II .\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "corpus[\"train_sentences\"] = sent_tokenize(corpus[\"train\"])\n",
    "corpus[\"dev_sentences\"] = sent_tokenize(corpus[\"dev\"])\n",
    "corpus[\"test_sentences\"] = sent_tokenize(corpus[\"test\"])\n",
    "\n",
    "print_sents(corpus[\"train_sentences\"], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valkyria', 'chronicles', 'iii', 'senjō', 'no', 'valkyria', '3', 'unrecorded', 'chronicles', 'japanese', '戦場のヴァルキュリア3', 'lit']\n",
      "___________________\n",
      "['valkyria', 'of', 'the', 'battlefield', '3', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', 'is', 'a', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media', 'vision', 'for', 'the', 'playstation', 'portable']\n",
      "___________________\n",
      "['released', 'in', 'january', '2011', 'in', 'japan', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series']\n",
      "___________________\n",
      "['employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', 'time', 'gameplay', 'as', 'its', 'predecessors', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', 'nameless', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', 'calamaty', 'raven']\n",
      "___________________\n",
      "['the', 'game', 'began', 'development', 'in', '2010', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii']\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "corpus[\"train_filtered\"] = [custom_tokenize(text) for text in corpus[\"train_sentences\"]]\n",
    "corpus[\"test_filtered\"] = [custom_tokenize(text) for text in corpus[\"test_sentences\"]]  \n",
    "corpus[\"dev_filtered\"] = [custom_tokenize(text) for text in corpus[\"dev_sentences\"]]\n",
    "\n",
    "print_sents(corpus[\"train_filtered\"], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens in train sentences: \n",
      "\n",
      "[('the', 130771),\n",
      " ('of', 57032),\n",
      " ('and', 50738),\n",
      " ('in', 45019),\n",
      " ('to', 39522),\n",
      " ('a', 36567),\n",
      " ('was', 21008),\n",
      " ('on', 15141),\n",
      " ('as', 15058),\n",
      " ('s', 14982),\n",
      " ('that', 14351),\n",
      " ('for', 13795),\n",
      " ('with', 13012),\n",
      " ('by', 12718),\n",
      " ('is', 11692),\n",
      " ('it', 9277),\n",
      " ('from', 9229),\n",
      " ('at', 9071),\n",
      " ('his', 9020),\n",
      " ('he', 8709)]\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution\n",
    "train_tokens = list(chain.from_iterable(corpus[\"train_filtered\"]))\n",
    "count = nltk.FreqDist(train_tokens)\n",
    "print('Top 20 most frequent tokens in train sentences: \\n')\n",
    "\n",
    "pprint(count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens in train sentences: \n",
      "\n",
      "[('the', 16083),\n",
      " ('of', 6789),\n",
      " ('and', 5885),\n",
      " ('in', 5079),\n",
      " ('to', 4787),\n",
      " ('a', 4090),\n",
      " ('was', 2575),\n",
      " ('on', 1903),\n",
      " ('as', 1605),\n",
      " ('that', 1522),\n",
      " ('s', 1516),\n",
      " ('for', 1490),\n",
      " ('with', 1485),\n",
      " ('by', 1478),\n",
      " ('he', 1329),\n",
      " ('at', 1240),\n",
      " ('his', 1179),\n",
      " ('is', 1137),\n",
      " ('were', 1085),\n",
      " ('from', 1061)]\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution\n",
    "test_tokens = list(chain.from_iterable(corpus[\"test_filtered\"]))\n",
    "count = nltk.FreqDist(test_tokens)\n",
    "print('Top 20 most frequent tokens in train sentences: \\n')\n",
    "\n",
    "pprint(count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens in train sentences: \n",
      "\n",
      "[('the', 14717),\n",
      " ('of', 5926),\n",
      " ('and', 5345),\n",
      " ('in', 4755),\n",
      " ('to', 4160),\n",
      " ('a', 3659),\n",
      " ('was', 2324),\n",
      " ('on', 1680),\n",
      " ('s', 1565),\n",
      " ('as', 1476),\n",
      " ('that', 1395),\n",
      " ('by', 1340),\n",
      " ('for', 1273),\n",
      " ('with', 1234),\n",
      " ('at', 1005),\n",
      " ('from', 973),\n",
      " ('is', 964),\n",
      " ('were', 886),\n",
      " ('it', 865),\n",
      " ('he', 817)]\n"
     ]
    }
   ],
   "source": [
    "# Frequency distribution\n",
    "dev_tokens = list(chain.from_iterable(corpus[\"dev_filtered\"]))\n",
    "count = nltk.FreqDist(dev_tokens)\n",
    "print('Top 20 most frequent tokens in train sentences: \\n')\n",
    "\n",
    "pprint(count.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function that returns a Unigram Counter of a given corpus.\"\"\"\n",
    "def calc_unigrams(tokens):\n",
    "  unigram_counter = Counter()\n",
    "  for sent in tokens:\n",
    "    unigram_counter.update([(gram,) for gram in [\"<s>\"] + sent])\n",
    "  \n",
    "  return unigram_counter\n",
    "\n",
    "\"\"\" Function that returns a Biagram Counter of a given corpus.\"\"\"\n",
    "def calc_bigrams(tokens):\n",
    "  bigram_counter = Counter()\n",
    "  for sent in tokens:\n",
    "    bigram_pad_sent = [\"<s>\"] + sent +  ['<e>']\n",
    "    bigram_counter.update([(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])])  \n",
    "\n",
    "  return bigram_counter\n",
    "\n",
    "\"\"\" Function that returns a Trigram Counter of a given corpus.\"\"\"\n",
    "def calc_trigrams(tokens):\n",
    "  trigram_counter = Counter()\n",
    "  for sent in tokens:\n",
    "    trigram_pad_sent = [\"<s>\"] + sent +  ['<e>']\n",
    "    trigram_counter.update([(gram1, gram2, gram3) for gram1, gram2, gram3 in zip(trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:])])  \n",
    "\n",
    "  return trigram_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_train(corpus):\n",
    "    \"\"\" Function that calculates and replaces OOV words.\n",
    "    INPUT: Train corpus (list)\n",
    "    OUTPUT: \n",
    "      OOV_word: dict with key containing OOC words and value the str 'UNK' -> dict\n",
    "      clean_corpus: the original corpus having the OOV words replaced by 'UNK' -> list\n",
    "      vocabulary: the words contained in the vocabulary -> set\n",
    "    \"\"\"\n",
    "\n",
    "    unigram_counter = calc_unigrams(corpus)\n",
    "    OOV_words = {k[0]:\"UNK\" for k, v in unigram_counter.items() if v < 10}\n",
    "    clean_corpus = []\n",
    "    for sentence in corpus:\n",
    "        clean_corpus.append([OOV_words.get(n,n) for n in sentence])\n",
    "    vocabulary = [f[0] for f in unigram_counter.keys() if f[0] not in OOV_words]\n",
    "    vocabulary = set(vocabulary) # set for unique words\n",
    "    return OOV_words, clean_corpus, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1322',\n",
       " 'yoganna',\n",
       " 'maxims',\n",
       " 'newsweek',\n",
       " 'frances',\n",
       " 'understandable',\n",
       " 'holyrood',\n",
       " 'anatol',\n",
       " 'qxd4',\n",
       " '2125']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_words, clean_corpus, vocabulary = replace_oov_words_train(corpus[\"train_filtered\"])\n",
    "random.sample(list(oov_words.keys()), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_dev_test(corpus, vocabulary, oov_words):\n",
    "    clean_corpus = []\n",
    "    for sentence in corpus:\n",
    "        updated_sentence = ['UNK' if ((word not in vocabulary) or (word in oov_words)) else word for word in sentence]\n",
    "        clean_corpus.append(updated_sentence)\n",
    "    return clean_corpus\n",
    "\n",
    "corpus[\"dev_filtered\"] = replace_oov_words_dev_test(corpus[\"dev_filtered\"], vocabulary, oov_words)\n",
    "corpus[\"test_filtered\"] = replace_oov_words_dev_test(corpus[\"test_filtered\"], vocabulary, oov_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_length = len(vocabulary)\n",
    "unigram_counter = calc_unigrams(corpus[\"train_filtered\"])\n",
    "bigram_counter = calc_bigrams(corpus[\"train_filtered\"])\n",
    "trigram_counter = calc_trigrams(corpus[\"train_filtered\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Length: 13130\n",
      "========================= \n",
      "Unigram 10 most common:\n",
      "[(('the',), 130771),\n",
      " (('<s>',), 78453),\n",
      " (('of',), 57032),\n",
      " (('and',), 50738),\n",
      " (('in',), 45019),\n",
      " (('to',), 39522),\n",
      " (('a',), 36567),\n",
      " (('was',), 21008),\n",
      " (('on',), 15141),\n",
      " (('as',), 15058)]\n",
      "========================= \n",
      "Bigram 10 most common:\n",
      "[(('of', 'the'), 17353),\n",
      " (('<s>', 'the'), 13695),\n",
      " (('in', 'the'), 11841),\n",
      " (('to', 'the'), 6031),\n",
      " (('<s>', 'in'), 4960),\n",
      " (('on', 'the'), 4518),\n",
      " (('and', 'the'), 4394),\n",
      " (('for', 'the'), 3729),\n",
      " (('at', 'the'), 3197),\n",
      " (('from', 'the'), 3014)]\n",
      "========================= \n",
      "Trigram 10 most common:\n",
      "[(('<s>', 'in', 'the'), 979),\n",
      " (('one', 'of', 'the'), 869),\n",
      " (('<s>', 'it', 'was'), 734),\n",
      " (('the', 'united', 'states'), 667),\n",
      " (('as', 'well', 'as'), 604),\n",
      " (('part', 'of', 'the'), 534),\n",
      " (('the', 'end', 'of'), 510),\n",
      " (('<s>', 'it', 'is'), 499),\n",
      " (('<s>', 'according', 'to'), 450),\n",
      " (('<s>', 'at', 'the'), 405)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary Length: {vocabulary_length}')\n",
    "print('='*25, '\\nUnigram 10 most common:')\n",
    "pprint(unigram_counter.most_common(10))\n",
    "print('='*25, '\\nBigram 10 most common:')\n",
    "pprint(bigram_counter.most_common(10))\n",
    "print('='*25, '\\nTrigram 10 most common:')\n",
    "pprint(trigram_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bi_prob(word1: str, word2: str, alpha: float, bigram_counter: Counter, unigram_counter: Counter, vocabulary_length: int) -> float:\n",
    "    \"\"\" Function that calculates the Bigram model's probabilities using Laplace & a-smoothing.\"\"\"\n",
    "    #Bigram prob + laplace smoothing\n",
    "    bigram_prob = (bigram_counter[(word1, word2)] + alpha) / (unigram_counter[(word1,)] + alpha * vocabulary_length)\n",
    "    return bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_LM(\n",
    "    corpus: list[list[str]],\n",
    "    unigram_counter: dict,\n",
    "    bigram_counter: dict\n",
    ") -> None:\n",
    "    vocab_size = len(unigram_counter)\n",
    "    alpha_list = np.linspace(0.001, 0.1, 100)\n",
    "\n",
    "    best_entropy = float(\"inf\")\n",
    "    best_alpha = None\n",
    "    best_perplexity = None\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        sum_log_prob = 0\n",
    "        bigram_cnt = 0\n",
    "\n",
    "        for sent in corpus:\n",
    "            sent = ['<s>'] + sent + ['<e>']\n",
    "\n",
    "            for i in range(1, len(sent)):\n",
    "                w1, w2 = sent[i - 1], sent[i]\n",
    "\n",
    "                # Skip prediction of start tokens\n",
    "                if w2 in {'<s>', '<s1>', '<s2>'}:\n",
    "                    continue\n",
    "\n",
    "                bigram_count = bigram_counter.get((w1, w2), 0)\n",
    "                unigram_count = unigram_counter.get(w1, 0)\n",
    "\n",
    "                prob = (bigram_count + alpha) / (unigram_count + alpha * vocab_size)\n",
    "                sum_log_prob += math.log2(prob)\n",
    "                bigram_cnt += 1\n",
    "\n",
    "        cross_entropy = -sum_log_prob / bigram_cnt\n",
    "        perplexity = math.pow(2, cross_entropy)\n",
    "\n",
    "        if cross_entropy < best_entropy:\n",
    "            best_entropy = cross_entropy\n",
    "            best_perplexity = perplexity\n",
    "            best_alpha = alpha\n",
    "\n",
    "    print(f\"Best Laplace-smoothed Bigram Model:\")\n",
    "    print(f\"  Cross Entropy: {best_entropy:.3f}\")\n",
    "    print(f\"  Perplexity:    {best_perplexity:.3f}\")\n",
    "    print(f\"  Best alpha:    {best_alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Laplace-smoothed Bigram Model:\n",
      "  Cross Entropy: 6.125\n",
      "  Perplexity:    69.796\n",
      "  Best alpha:    0.001\n"
     ]
    }
   ],
   "source": [
    "bigram_LM(corpus[\"dev_filtered\"], unigram_counter, bigram_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tri_prob(word1: str, word2: str, word3:str, alpha: float, trigram_counter:Counter, bigram_counter:Counter, vocabulary_length: int) -> float:\n",
    "    \"\"\" Function that calculates the Trigram model's probabilities using Laplace & a-smoothing.\"\"\"\n",
    "    #Bigram prob + laplace smoothing\n",
    "    trigram_prob = (trigram_counter[(word1, word2, word3)] +alpha) / (bigram_counter[(word1, word2)] + alpha * vocabulary_length)\n",
    "    return trigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_LM(\n",
    "    corpus: list[list[str]],\n",
    "    bigram_counter: dict,\n",
    "    trigram_counter: dict,\n",
    "    unigram_counter: dict\n",
    ") -> None:\n",
    "    vocab_size = len(unigram_counter)\n",
    "    alpha_list = np.linspace(0.001, 0.1, 100)\n",
    "\n",
    "    # Initialize best metrics\n",
    "    best_entropy = float(\"inf\")\n",
    "    best_perplexity = None\n",
    "    best_alpha = None\n",
    "\n",
    "    # Add pseudo bigram (<s>, <s>)\n",
    "    bigram_counter[(\"<s>\", \"<s>\")] = len(corpus)\n",
    "\n",
    "    for alpha in alpha_list:\n",
    "        sum_log_prob = 0\n",
    "        trigram_cnt = 0\n",
    "\n",
    "        for sent in corpus:\n",
    "            sent = ['<s>', '<s>'] + sent + ['<e>']\n",
    "\n",
    "            for i in range(2, len(sent)):\n",
    "                w1, w2, w3 = sent[i-2], sent[i-1], sent[i]\n",
    "\n",
    "                # Skip if predicting a start token\n",
    "                if w3 in {'<s>', '<s1>', '<s2>'}:\n",
    "                    continue\n",
    "\n",
    "                trigram_count = trigram_counter.get((w1, w2, w3), 0)\n",
    "                bigram_count = bigram_counter.get((w1, w2), 0)\n",
    "\n",
    "                prob = (trigram_count + alpha) / (bigram_count + alpha * vocab_size)\n",
    "                sum_log_prob += math.log2(prob)\n",
    "                trigram_cnt += 1\n",
    "\n",
    "        cross_entropy = -sum_log_prob / trigram_cnt\n",
    "        perplexity = math.pow(2, cross_entropy)\n",
    "\n",
    "        # Track best alpha\n",
    "        if cross_entropy < best_entropy:\n",
    "            best_entropy = cross_entropy\n",
    "            best_perplexity = perplexity\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # Final results\n",
    "    print(\"Trigram LM (Laplace-smoothed) — Alpha Tuning:\")\n",
    "    print(f\"  Best alpha:      {best_alpha:.3f}\")\n",
    "    print(f\"  Cross Entropy:   {best_entropy:.3f}\")\n",
    "    print(f\"  Perplexity:      {best_perplexity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram LM (Laplace-smoothed) — Alpha Tuning:\n",
      "  Best alpha:      0.001\n",
      "  Cross Entropy:   13.866\n",
      "  Perplexity:      14928.122\n"
     ]
    }
   ],
   "source": [
    "trigram_LM(corpus[\"dev_filtered\"], bigram_counter, trigram_counter, unigram_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

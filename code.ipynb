{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (4.51.2)\n",
      "Requirement already satisfied: tiktoken in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install datasets\n",
    "%pip install -U nltk\n",
    "%pip install transformers tiktoken\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, TweetTokenizer\n",
    "from transformers import BertTokenizer\n",
    "from itertools import chain\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "import string\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set #tokens: 2051910\n",
      "Valid set #tokens: 213886\n",
      "Test set #tokens: 241211\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Convert splits into plain text strings\n",
    "data = {\n",
    "    \"train\": \" \".join(raw_dataset[\"train\"][\"text\"]),\n",
    "    \"valid\": \" \".join(raw_dataset[\"validation\"][\"text\"]),\n",
    "    \"test\":  \" \".join(raw_dataset[\"test\"][\"text\"]),\n",
    "}\n",
    "\n",
    "# Display token counts\n",
    "for split in data:\n",
    "    print(f\"{split.capitalize()} set #tokens: {len(data[split].split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sents(sents: list, n_first:int) -> None:\n",
    "  for sent in sents[:n_first]:\n",
    "    print(sent)\n",
    "    print(\"___________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  = Valkyria Chronicles III = \n",
      "   Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit .\n",
      "___________________\n",
      "Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable .\n",
      "___________________\n",
      "Released in January 2011 in Japan , it is the third game in the Valkyria series .\n",
      "___________________\n",
      "Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .\n",
      "___________________\n",
      "The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II .\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "train_sentences = sent_tokenize(data[\"train\"])\n",
    "valid_sentences = sent_tokenize(data[\"valid\"])\n",
    "test_sentences = sent_tokenize(data[\"test\"])\n",
    "\n",
    "print_sents(train_sentences, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentences, tokenizer):\n",
    "  \"\"\"Tokenize sentences using a specified tokenizer.\"\"\"\n",
    "  tokens = []\n",
    "  for sent in sentences:\n",
    "      tokens.append(tokenizer.tokenize(sent.lower()))\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "___________________\n",
      "[]\n",
      "___________________\n",
      "['=']\n",
      "___________________\n",
      "[]\n",
      "___________________\n",
      "['v']\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tokens = get_tokens(train_sentences, tokenizer)\n",
    "print_sents(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valkyria', 'chronicles', 'iii', 'senjō', 'no', 'valkyria', '3', 'unrecorded', 'chronicles', '(', 'japanese', '戦場のヴァルキュリア3', ',', 'lit', '.']\n",
      "___________________\n",
      "['valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media', '.', 'vision', 'for', 'the', 'playstation', 'portable', '.']\n",
      "___________________\n",
      "['released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.']\n",
      "___________________\n",
      "['employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', 'nameless', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gallia', 'during', 'the', 'second', 'europan', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', 'calamaty', 'raven', '.']\n",
      "___________________\n",
      "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'valkyria', 'chronicles', 'ii', '.']\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(pattern='\\w+|\\(|\\)|\\.|\\,')\n",
    "\n",
    "tokens = get_tokens(train_sentences, tokenizer)\n",
    "print_sents(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'val', '##ky', '##ria', 'chronicles', 'iii', '=', 'sen', '##jo', 'no', 'val', '##ky', '##ria', '3', ':', 'un', '##re', '##cor', '##ded', 'chronicles', '(', 'japanese', ':', '戦', '場', 'の', '##ウ', '##ァ', '##ル', '##キ', '##ュ', '##リ', '##ア', '##3', ',', 'lit', '.']\n",
      "___________________\n",
      "['val', '##ky', '##ria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'val', '##ky', '##ria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@', '-', '@', 'playing', 'video', 'game', 'developed', 'by', 'sega', 'and', 'media', '.', 'vision', 'for', 'the', 'playstation', 'portable', '.']\n",
      "___________________\n",
      "['released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'val', '##ky', '##ria', 'series', '.']\n",
      "___________________\n",
      "['employing', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@', '-', '@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'name', '##less', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gall', '##ia', 'during', 'the', 'second', 'europa', '##n', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitted', 'against', 'the', 'imperial', 'unit', '\"', 'cal', '##ama', '##ty', 'raven', '\"', '.']\n",
      "___________________\n",
      "['the', 'game', 'began', 'development', 'in', '2010', ',', 'carrying', 'over', 'a', 'large', 'portion', 'of', 'the', 'work', 'done', 'on', 'val', '##ky', '##ria', 'chronicles', 'ii', '.']\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_tokens = get_tokens(train_sentences, tokenizer)\n",
    "print_sents(tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens = get_tokens(valid_sentences, tokenizer)\n",
    "print_sents(valid_tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['=', 'robert', 'bo', '##ult', '##er', '=', 'robert', 'bo', '##ult', '##er', 'is', 'an', 'english', 'film', ',', 'television', 'and', 'theatre', 'actor', '.']\n",
      "___________________\n",
      "['he', 'had', 'a', 'guest', '@', '-', '@', 'starring', 'role', 'on', 'the', 'television', 'series', 'the', 'bill', 'in', '2000', '.']\n",
      "___________________\n",
      "['this', 'was', 'followed', 'by', 'a', 'starring', 'role', 'in', 'the', 'play', 'heron', '##s', 'written', 'by', 'simon', 'stephens', ',', 'which', 'was', 'performed', 'in', '2001', 'at', 'the', 'royal', 'court', 'theatre', '.']\n",
      "___________________\n",
      "['he', 'had', 'a', 'guest', 'role', 'in', 'the', 'television', 'series', 'judge', 'john', 'deed', 'in', '2002', '.']\n",
      "___________________\n",
      "['in', '2004', 'bo', '##ult', '##er', 'landed', 'a', 'role', 'as', '\"', 'craig', '\"', 'in', 'the', 'episode', '\"', 'teddy', \"'\", 's', 'story', '\"', 'of', 'the', 'television', 'series', 'the', 'long', 'firm', ';', 'he', 'starred', 'alongside', 'actors', 'mark', 'strong', 'and', 'derek', 'jacob', '##i', '.']\n",
      "___________________\n"
     ]
    }
   ],
   "source": [
    "test_tokens = get_tokens(test_sentences, tokenizer)\n",
    "print_sents(test_tokens, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent tokens: \n",
      "\n",
      "[('the', 130865),\n",
      " (',', 102624),\n",
      " ('.', 84291),\n",
      " ('of', 57044),\n",
      " ('and', 50809),\n",
      " ('@', 45600),\n",
      " ('in', 45498),\n",
      " ('to', 39699),\n",
      " ('a', 36731),\n",
      " ('=', 29570),\n",
      " ('\"', 28309),\n",
      " ('was', 21026),\n",
      " (\"'\", 18655),\n",
      " ('-', 17337),\n",
      " ('as', 15386),\n",
      " ('on', 15182),\n",
      " ('s', 15120),\n",
      " ('that', 14374),\n",
      " ('for', 13878),\n",
      " ('with', 13043)]\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "\n",
    "# Frequency distribution\n",
    "train_tokens = list(chain.from_iterable(tokens))\n",
    "count = nltk.FreqDist(train_tokens)\n",
    "print('Top 20 most frequent tokens in train sentences: \\n')\n",
    "\n",
    "pprint(count.most_common(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens_by_sentence(sentences, min_token_length=8, top_n=20, max_tokens=300000):\n",
    "    \"\"\"\n",
    "    Filters tokens in each sentence based on stopwords and length,\n",
    "    and prints the top-N most frequent remaining tokens.\n",
    "\n",
    "    Parameters:\n",
    "        sentences: List of tokenized sentences (list of lists of str)\n",
    "        min_token_length: Minimum length of token to keep\n",
    "        top_n: Number of most common tokens to print\n",
    "        max_tokens: Total tokens to consider (flat, from the start)\n",
    "\n",
    "    Returns:\n",
    "        filtered_sents: List of filtered tokenized sentences (same format as input)\n",
    "    \"\"\"\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    stop.update(string.punctuation)\n",
    "    stop.update([\"the\", \"of\", \"and\", \"as\", \"a\", \"to\", \"in\", \"on\", \"for\"])\n",
    "\n",
    "    # Flatten, limit, and filter\n",
    "    flat_tokens = list(chain.from_iterable(sentences))[:max_tokens]\n",
    "    filtered_flat = [t for t in flat_tokens if t not in stop and len(t) >= min_token_length]\n",
    "\n",
    "    # Print top-N tokens\n",
    "    print(f\"\\nTop {top_n} most frequent filtered tokens:\\n\")\n",
    "    count = FreqDist(filtered_flat)\n",
    "    pprint(count.most_common(top_n))\n",
    "\n",
    "    # Now filter sentence by sentence for structured n-gram use\n",
    "    filtered_sents = [\n",
    "        [t for t in sent if t not in stop and len(t) >= min_token_length]\n",
    "        for sent in sentences\n",
    "    ]\n",
    "\n",
    "    return filtered_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 most frequent filtered tokens:\n",
      "\n",
      "[('although', 191),\n",
      " ('galveston', 191),\n",
      " ('including', 156),\n",
      " ('townsend', 150),\n",
      " ('released', 136),\n",
      " ('national', 133),\n",
      " ('following', 122),\n",
      " ('described', 119),\n",
      " ('american', 116),\n",
      " ('aircraft', 115),\n",
      " ('november', 112),\n",
      " ('university', 108),\n",
      " ('christmas', 107),\n",
      " ('received', 106),\n",
      " ('december', 97),\n",
      " ('original', 95),\n",
      " ('september', 93),\n",
      " ('government', 93),\n",
      " ('included', 91),\n",
      " ('building', 90)]\n"
     ]
    }
   ],
   "source": [
    "train_tokens_filtered = filter_tokens_by_sentence(train_tokens, min_token_length=8, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78453/78453 [00:00<00:00, 214466.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('<s>',), 78453),\n",
      " (('including',), 1273),\n",
      " (('although',), 1194),\n",
      " (('following',), 1152),\n",
      " (('american',), 1025),\n",
      " (('released',), 979),\n",
      " (('national',), 925),\n",
      " (('september',), 869),\n",
      " (('received',), 785),\n",
      " (('government',), 774)]\n",
      "\n",
      "[(('<s>', '<e>'), 9050),\n",
      " (('<s>', 'following'), 587),\n",
      " (('<s>', 'although'), 586),\n",
      " (('<s>', 'according'), 518),\n",
      " (('<s>', 'released'), 418),\n",
      " (('<s>', 'received'), 350),\n",
      " (('including', '<e>'), 337),\n",
      " (('released', '<e>'), 333),\n",
      " (('<s>', 'described'), 322),\n",
      " (('september', '<e>'), 310)]\n",
      "\n",
      "[(('<s>', '<s>', '<e>'), 9050),\n",
      " (('<s>', '<e>', '<e>'), 9050),\n",
      " (('<s>', '<s>', 'following'), 587),\n",
      " (('<s>', '<s>', 'although'), 586),\n",
      " (('<s>', '<s>', 'according'), 518),\n",
      " (('<s>', '<s>', 'released'), 418),\n",
      " (('<s>', '<s>', 'received'), 350),\n",
      " (('including', '<e>', '<e>'), 337),\n",
      " (('released', '<e>', '<e>'), 333),\n",
      " (('<s>', '<s>', 'described'), 322)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "unigram_counter = Counter()\n",
    "bigram_counter = Counter()\n",
    "trigram_counter = Counter()\n",
    "\n",
    "for sent in tqdm(train_tokens_filtered):\n",
    "\n",
    "    # Update the unigram counter\n",
    "    unigram_counter.update([(gram,) for gram in [\"<s>\"] + sent])\n",
    "\n",
    "    # Update the bigram counter\n",
    "    bigram_pad_sent = [\"<s>\"] + sent +  ['<e>']\n",
    "    bigram_counter.update([(gram1, gram2) for gram1, gram2 in zip(bigram_pad_sent, bigram_pad_sent[1:])])\n",
    "\n",
    "    # Update the trigram counter\n",
    "    trigram_pad_sent = [\"<s>\"]*2 + sent +  ['<e>']*2\n",
    "    trigram_counter.update([(gram1, gram2, gram3) for gram1, gram2, gram3 in zip(trigram_pad_sent, trigram_pad_sent[1:], trigram_pad_sent[2:])])\n",
    "\n",
    "\n",
    "pprint(unigram_counter.most_common(10))\n",
    "print()\n",
    "pprint(bigram_counter.most_common(10))\n",
    "print()\n",
    "pprint(trigram_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_train(corpus):\n",
    "    \"\"\" Function that calculates and replaces OOV words.\n",
    "    INPUT: Train corpus (list)\n",
    "    OUTPUT: \n",
    "      OOV_word: dict with key containing OOC words and value the str 'UNK' -> dict\n",
    "      clean_corpus: the original corpus having the OOV words replaced by 'UNK' -> list\n",
    "      vocabulary: the words contained in the vocabulary -> set\n",
    "    \"\"\"\n",
    "\n",
    "    unigram_counter = calc_unigrams(corpus)\n",
    "    OOV_words = {k[0]:\"UNK\" for k, v in unigram_counter.items() if v < 10}\n",
    "    clean_corpus = []\n",
    "    for sentence in corpus:\n",
    "        clean_corpus.append([OOV_words.get(n,n) for n in sentence])\n",
    "    vocabulary = [f[0] for f in unigram_counter.keys() if f[0] not in OOV_words]\n",
    "    vocabulary = set(vocabulary) # set for unique words\n",
    "    return OOV_words, clean_corpus, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_unigrams(corpus: list) -> Counter:\n",
    "    \"\"\" Function that returns a Unigram Counter of a given corpus.\"\"\"\n",
    "    unigram_counter = Counter()\n",
    "    for sentence in corpus:\n",
    "        unigram_counter.update([gram for gram in ngrams(sentence, 1, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    return unigram_counter\n",
    "\n",
    "def calc_bigrams(corpus: list) -> Counter:\n",
    "    \"\"\" Function that returns a Biagram Counter of a given corpus.\"\"\"\n",
    "    bigram_counter = Counter()\n",
    "    for sentence in corpus:\n",
    "        bigram_counter.update([gram for gram in ngrams(sentence, 2, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    return bigram_counter\n",
    "\n",
    "\n",
    "def calc_trigrams(corpus: list) -> Counter:\n",
    "    \"\"\" Function that returns a Trigram Counter of a given corpus.\"\"\"\n",
    "    trigram_counter = Counter()\n",
    "    for sentence in corpus:\n",
    "        trigram_counter.update([gram for gram in ngrams(sentence, 3, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='<e>')])\n",
    "    return trigram_counter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c343ddb",
   "metadata": {},
   "source": [
    "# Numerical Optimization and Large Scale Linear Algebra\n",
    "## Project 3: PageRank Algorithm Implementation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42e75a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.sparse import identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e06585",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We begin by loading the Stanford web connectivity data, which represents the hyperlink structure between Stanford University webpages. The data consists of directed edges where each row contains a source node and target node, representing a hyperlink from one page to another.\n",
    "\n",
    "The preprocessing step creates a zero-indexed mapping of all unique nodes to ensure efficient matrix operations in subsequent calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd72ba6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/27/w24km3fs2vl7bm7hlvrwm2l40000gn/T/ipykernel_61915/2698559121.py:7: DeprecationWarning: loadtxt(): Parsing an integer via a float is deprecated.  To avoid this warning, you can:\n",
      "    * make sure the original data is stored as integers.\n",
      "    * use the `converters=` keyword argument.  If you only use\n",
      "      NumPy 1.23 or later, `converters=float` will normally work.\n",
      "    * Use `np.loadtxt(...).astype(np.int64)` parsing the file as\n",
      "      floating point and then convert it.  (On all NumPy versions.)\n",
      "  (Deprecated NumPy 1.23)\n",
      "  data = np.loadtxt(f, dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2382912 edges, 281903 nodes\n"
     ]
    }
   ],
   "source": [
    "# Load web connectivity data from zip file and create node mappings\n",
    "def load_web_data(filename):\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_file:\n",
    "            dat_filename = zip_file.namelist()[0]\n",
    "            with zip_file.open(dat_filename) as f:\n",
    "                data = np.loadtxt(f, dtype=int)\n",
    "    else:\n",
    "        data = np.loadtxt(filename, dtype=int)\n",
    "  \n",
    "    sources = data[:, 0]\n",
    "    targets = data[:, 1]\n",
    "    \n",
    "    all_nodes = np.unique(np.concatenate([sources, targets]))\n",
    "    n = len(all_nodes)\n",
    "    node_to_idx = {node: i for i, node in enumerate(all_nodes)}\n",
    "    \n",
    "    sources_idx = np.array([node_to_idx[s] for s in sources])\n",
    "    targets_idx = np.array([node_to_idx[t] for t in targets])\n",
    "    \n",
    "    return sources_idx, targets_idx, n, all_nodes\n",
    "\n",
    "sources, targets, n, node_list = load_web_data('stanweb.dat.zip')\n",
    "print(f\"{len(sources)} edges, {n} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e314ad6",
   "metadata": {},
   "source": [
    "## Transition Matrix Construction\n",
    "\n",
    "The PageRank algorithm requires constructing a sparse transition matrix **P** from the web graph structure. Each entry P[i,j] represents the probability of transitioning from page i to page j.\n",
    "\n",
    "- **Outlink normalization**: Each row sums to 1 by dividing by the number of outgoing links\n",
    "- **Dangling nodes**: Pages with no outlinks are identified in vector **a** for special handling\n",
    "- **Sparse representation**: Using CSR format for memory efficiency with large graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6a4d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (281903, 281903)\n",
      "Dangling nodes: 172\n"
     ]
    }
   ],
   "source": [
    "# Build sparse transition matrix P and dangling nodes vector a\n",
    "def build_transition_matrix(sources, targets, n):\n",
    "    outlinks = np.bincount(sources, minlength=n)\n",
    "    a = (outlinks == 0).astype(float)\n",
    "    weights = 1.0 / outlinks[sources]\n",
    "    P = csr_matrix((weights, (sources, targets)), shape=(n, n))\n",
    "    return P, a\n",
    "\n",
    "P, a = build_transition_matrix(sources, targets, n)\n",
    "print(f\"Matrix shape: {P.shape}\")\n",
    "print(f\"Dangling nodes: {np.sum(a):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3e16b3",
   "metadata": {},
   "source": [
    "## Part A: PageRank via Power Method\n",
    "\n",
    "The power method is the traditional approach for computing PageRank. It iteratively applies the PageRank equation until convergence:\n",
    "\n",
    "**π^(k+1) = α π^(k) P + (α π^(k) a + (1-α)) v^T**\n",
    "\n",
    "Where:\n",
    "- **α = 0.85**: Damping factor balancing link structure vs. random teleportation\n",
    "- **Convergence criterion**: ||π^(k+1) - π^(k)||₁ < 10⁻⁸\n",
    "- **Matrix-free implementation**: Avoids storing the dense Google matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f494f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 iterations, 0.4580 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank using power method implementation\n",
    "def pagerank_power_method(P, a, alpha=0.85, tol=1e-8, max_iter=1000):\n",
    "    n = P.shape[0]\n",
    "    x = np.ones(n) / n\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        x_old = x.copy()\n",
    "        x_new = alpha * (x @ P)\n",
    "        x_new += alpha * np.dot(x, a) / n\n",
    "        x_new += (1 - alpha) / n\n",
    "        x = x_new\n",
    "        \n",
    "        if np.linalg.norm(x - x_old, 1) < tol:\n",
    "            return x, iteration + 1\n",
    "    \n",
    "    return x, max_iter\n",
    "\n",
    "start_time = time.time()\n",
    "pi_power, power_iterations = pagerank_power_method(P, a, alpha=0.85)\n",
    "power_time = time.time() - start_time\n",
    "print(f\"{power_iterations} iterations, {power_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4d4e7",
   "metadata": {},
   "source": [
    "## Alternative: Linear System Formulation\n",
    "\n",
    "Instead of the iterative power method, PageRank can be computed by solving the linear system directly:\n",
    "\n",
    "**(I - αP^T)π = (1-α)/n e**\n",
    "\n",
    "This approach:\n",
    "- **Direct solution**: No iteration required, single solve operation\n",
    "- **Sparse solver**: Uses efficient sparse linear algebra routines\n",
    "- **Guaranteed convergence**: No iteration count dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "780165f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0613 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute PageRank by solving linear system (I - αP^T)π = (1-α)/n * e\n",
    "def pagerank_linear_system(P, a, alpha=0.85):\n",
    "    n = P.shape[0]\n",
    "    I = identity(n, format='csr')\n",
    "    A = I - alpha * P.T\n",
    "    b = (1 - alpha) / n * np.ones(n)\n",
    "    pi = spsolve(A, b)\n",
    "    return pi / np.sum(pi)\n",
    "\n",
    "start_time = time.time()\n",
    "pi_linear = pagerank_linear_system(P, a, alpha=0.85)\n",
    "linear_time = time.time() - start_time\n",
    "print(f\"{linear_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d88d5b",
   "metadata": {},
   "source": [
    "## Method Comparison and Results\n",
    "\n",
    "Comparing the computational efficiency and accuracy of both approaches provides insights into their practical trade-offs. The L1 norm difference measures numerical accuracy between methods.\n",
    "\n",
    "The top-ranked pages represent the most \"important\" nodes in Stanford's web graph according to the PageRank algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fefde07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power method: 91 iterations, 0.4580s\n",
      "Linear system: 15.0613s\n",
      "L1 difference: 1.06e-08\n",
      "\n",
      "Top 10 PageRank nodes:\n",
      " 1. Node 89073: 0.01130284\n",
      " 2. Node 226411: 0.00928766\n",
      " 3. Node 241454: 0.00829723\n",
      " 4. Node 262860: 0.00302312\n",
      " 5. Node 134832: 0.00300127\n",
      " 6. Node 234704: 0.00257226\n",
      " 7. Node 136821: 0.00245370\n",
      " 8. Node 68889: 0.00243078\n",
      " 9. Node 105607: 0.00239743\n",
      "10. Node 69358: 0.00236400\n"
     ]
    }
   ],
   "source": [
    "# Compare results from both methods and display top-ranked pages\n",
    "difference = np.linalg.norm(pi_power - pi_linear, 1)\n",
    "\n",
    "print(f\"Power method: {power_iterations} iterations, {power_time:.4f}s\")\n",
    "print(f\"Linear system: {linear_time:.4f}s\")\n",
    "print(f\"L1 difference: {difference:.2e}\")\n",
    "\n",
    "top_10_indices = np.argsort(pi_power)[-10:][::-1]\n",
    "print(f\"\\nTop 10 PageRank nodes:\")\n",
    "for i, idx in enumerate(top_10_indices):\n",
    "    print(f\"{i+1:2d}. Node {node_list[idx]}: {pi_power[idx]:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b398952",
   "metadata": {},
   "source": [
    "## Part B: Impact of α = 0.99\n",
    "\n",
    "Increasing α closer to 1.0 emphasizes the actual link structure over random teleportation, but significantly affects convergence properties. The subdominant eigenvalue of the Google matrix approaches 1, slowing power method convergence dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb280fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power method: 1392 iterations, 5.4780s\n",
      "Linear system: 6.9281s\n",
      "Slowdown factor: 15.3x\n"
     ]
    }
   ],
   "source": [
    "# Test convergence behavior with α = 0.99\n",
    "start_time = time.time()\n",
    "pi_power_99, power_iterations_99 = pagerank_power_method(P, a, alpha=0.99, max_iter=2000)\n",
    "power_time_99 = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "pi_linear_99 = pagerank_linear_system(P, a, alpha=0.99)\n",
    "linear_time_99 = time.time() - start_time\n",
    "\n",
    "print(f\"Power method: {power_iterations_99} iterations, {power_time_99:.4f}s\")\n",
    "print(f\"Linear system: {linear_time_99:.4f}s\")\n",
    "print(f\"Slowdown factor: {power_iterations_99/power_iterations:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8e6124",
   "metadata": {},
   "source": [
    "## Ranking Stability Analysis\n",
    "\n",
    "Different α values can produce significantly different PageRank orderings. Higher α values give more weight to the actual hyperlink structure, potentially promoting different pages to prominence.\n",
    "\n",
    "The overlap in top-50 rankings quantifies how sensitive the algorithm is to the damping parameter choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8811f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common nodes in top 50: 25/50\n",
      "\n",
      "Top 10 (α = 0.99):\n",
      " 1. Node 89073: 0.00918678 (α=0.85 rank: 1)\n",
      " 2. Node 281772: 0.00911239 (α=0.85 rank: Not in top 50)\n",
      " 3. Node 174665: 0.00768853 (α=0.85 rank: Not in top 50)\n",
      " 4. Node 226411: 0.00451447 (α=0.85 rank: 2)\n",
      " 5. Node 179645: 0.00407272 (α=0.85 rank: 21)\n",
      " 6. Node 271409: 0.00387233 (α=0.85 rank: Not in top 50)\n",
      " 7. Node 262860: 0.00348533 (α=0.85 rank: 4)\n",
      " 8. Node 136821: 0.00282083 (α=0.85 rank: 7)\n",
      " 9. Node 68889: 0.00279021 (α=0.85 rank: 8)\n",
      "10. Node 77988: 0.00267625 (α=0.85 rank: Not in top 50)\n"
     ]
    }
   ],
   "source": [
    "# Compare rankings between α = 0.85 and α = 0.99\n",
    "top_50_indices_85 = np.argsort(pi_power)[-50:][::-1]\n",
    "top_50_indices_99 = np.argsort(pi_power_99)[-50:][::-1]\n",
    "common_top_50 = len(set(top_50_indices_85) & set(top_50_indices_99))\n",
    "\n",
    "print(f\"Common nodes in top 50: {common_top_50}/50\")\n",
    "\n",
    "print(f\"\\nTop 10 (α = 0.99):\")\n",
    "for i, idx in enumerate(top_50_indices_99[:10]):\n",
    "    rank_in_85 = np.where(top_50_indices_85 == idx)[0]\n",
    "    rank_85 = rank_in_85[0] + 1 if len(rank_in_85) > 0 else \"Not in top 50\"\n",
    "    print(f\"{i+1:2d}. Node {node_list[idx]}: {pi_power_99[idx]:.8f} (α=0.85 rank: {rank_85})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5341b26",
   "metadata": {},
   "source": [
    "## Part C: Component-wise Convergence Analysis\n",
    "\n",
    "The power method doesn't converge uniformly across all PageRank components. Different nodes may reach their final values at different rates, depending on their position in the web graph structure.\n",
    "\n",
    "This analysis tracks convergence behavior of high-importance vs. low-importance nodes to understand the algorithm's dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "398a74ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest importance: [89073, 226411, 241454, 262860, 134832]\n",
      "Lowest importance: [1, 100721, 216231, 216228, 23318]\n"
     ]
    }
   ],
   "source": [
    "# Analyze component-wise convergence behavior\n",
    "def analyze_convergence(P, a, alpha=0.85, tol=1e-8):\n",
    "    n = P.shape[0]\n",
    "    x = np.ones(n) / n\n",
    "    convergence_history = []\n",
    "    \n",
    "    for iteration in range(500):\n",
    "        x_old = x.copy()\n",
    "        x_new = alpha * (x @ P)\n",
    "        x_new += alpha * np.dot(x, a) / n\n",
    "        x_new += (1 - alpha) / n\n",
    "        x = x_new\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            convergence_history.append(x.copy())\n",
    "        \n",
    "        if np.linalg.norm(x - x_old, 1) < tol:\n",
    "            break\n",
    "    \n",
    "    return x, convergence_history, iteration + 1\n",
    "\n",
    "pi_85, history_85, iters_85 = analyze_convergence(P, a, alpha=0.85)\n",
    "pi_99, history_99, iters_99 = analyze_convergence(P, a, alpha=0.99)\n",
    "\n",
    "top_5_nodes = np.argsort(pi_85)[-5:][::-1]\n",
    "bottom_5_nodes = np.argsort(pi_85)[:5]\n",
    "print(f\"Highest importance: {[node_list[i] for i in top_5_nodes]}\")\n",
    "print(f\"Lowest importance: {[node_list[i] for i in bottom_5_nodes]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b53cf",
   "metadata": {},
   "source": [
    "## Convergence Insights\n",
    "\n",
    "The analysis reveals fundamental differences between iterative and direct solution approaches, with important implications for large-scale PageRank computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff9279d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "α = 0.85 - Iterations to 1% accuracy:\n",
      "  High importance nodes: 20\n",
      "  Low importance nodes: 0\n",
      "α = 0.99 - Iterations to 1% accuracy:\n",
      "  High importance nodes: 230\n",
      "  Low importance nodes: 30\n"
     ]
    }
   ],
   "source": [
    "# Analyze convergence speed for different node types\n",
    "def convergence_analysis(history, final_pi, top_nodes, bottom_nodes, alpha):\n",
    "    iterations = len(history)\n",
    "    top_errors = np.zeros(iterations)\n",
    "    bottom_errors = np.zeros(iterations)\n",
    "    \n",
    "    for i, pi_iter in enumerate(history):\n",
    "        top_errors[i] = np.mean([abs(pi_iter[node] - final_pi[node]) / final_pi[node] \n",
    "                                for node in top_nodes])\n",
    "        bottom_errors[i] = np.mean([abs(pi_iter[node] - final_pi[node]) / final_pi[node] \n",
    "                                  for node in bottom_nodes])\n",
    "    \n",
    "    top_converge = np.where(top_errors < 0.01)[0]\n",
    "    bottom_converge = np.where(bottom_errors < 0.01)[0]\n",
    "    \n",
    "    top_iter = top_converge[0] * 10 if len(top_converge) > 0 else \"No convergence\"\n",
    "    bottom_iter = bottom_converge[0] * 10 if len(bottom_converge) > 0 else \"No convergence\"\n",
    "    \n",
    "    print(f\"α = {alpha} - Iterations to 1% accuracy:\")\n",
    "    print(f\"  High importance nodes: {top_iter}\")\n",
    "    print(f\"  Low importance nodes: {bottom_iter}\")\n",
    "\n",
    "convergence_analysis(history_85, pi_85, top_5_nodes, bottom_5_nodes, 0.85)\n",
    "convergence_analysis(history_99, pi_99, top_5_nodes, bottom_5_nodes, 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3927c",
   "metadata": {},
   "source": [
    "## Part D: Link Farm Analysis\n",
    "\n",
    "### D.1: Adding an Isolated Page\n",
    "\n",
    "When adding a new page X with no incoming or outgoing links to an existing web graph, the PageRank redistribution follows mathematical principles. The new page becomes a \"dangling node\" that only receives PageRank through the teleportation component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c92c5dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding isolated page X:\n",
      "Original pages: 0.999996 × original PageRank\n",
      "New page X: 0.000004\n"
     ]
    }
   ],
   "source": [
    "# D.1: Mathematical analysis of adding isolated page X\n",
    "n_orig = n\n",
    "print(f\"Adding isolated page X:\")\n",
    "print(f\"Original pages: {n_orig/(n_orig+1):.6f} × original PageRank\")\n",
    "print(f\"New page X: {1/(n_orig+1):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5cd8f",
   "metadata": {},
   "source": [
    "### D.2: Strategic Link Addition\n",
    "\n",
    "Creating page Y that links exclusively to X demonstrates how incoming links can boost PageRank. The improvement factor quantifies the benefit of having an incoming link versus remaining completely isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6cd32de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y PageRank: 0.00000053\n",
      "X PageRank: 0.00000098\n",
      "Improvement factor: 0.278\n"
     ]
    }
   ],
   "source": [
    "# D.2: Adding page Y that links to X\n",
    "def analyze_Y_links_to_X(n_original, alpha=0.85):\n",
    "    y_pagerank = (1-alpha) / (n_original + 2)\n",
    "    x_pagerank = (1-alpha) / (n_original + 2) + alpha * y_pagerank\n",
    "    isolated_x_rank = 1 / (n_original + 2)\n",
    "    \n",
    "    print(f\"Y PageRank: {y_pagerank:.8f}\")\n",
    "    print(f\"X PageRank: {x_pagerank:.8f}\")\n",
    "    print(f\"Improvement factor: {x_pagerank / isolated_x_rank:.3f}\")\n",
    "\n",
    "analyze_Y_links_to_X(n, alpha=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da4017",
   "metadata": {},
   "source": [
    "### D.3-D.5: Link Farm Optimization Strategies\n",
    "\n",
    "#### D.3: Optimal Link Structure for Three Pages (X, Y, Z)\n",
    "\n",
    "The most effective configuration for maximizing page X's PageRank involves creating a **directed link farm**:\n",
    "\n",
    "- **Y → X only** (Y has no other outlinks)\n",
    "- **Z → X only** (Z has no other outlinks)  \n",
    "- **X remains a dangling node** (no outlinks)\n",
    "\n",
    "This structure concentrates all PageRank flow from Y and Z directly into X, maximizing the target page's importance score.\n",
    "\n",
    "#### D.4: Counter-intuitive Effect of Outlinks\n",
    "\n",
    "Adding outlinks from X to popular pages **reduces** X's PageRank due to the PageRank distribution mechanism:\n",
    "\n",
    "- **PageRank dilution**: X's accumulated PageRank gets distributed among all its outlinks\n",
    "- **Optimal strategy**: Keep X as a dangling node to retain all incoming PageRank\n",
    "- **Trade-off**: While outlinks might improve user experience, they harm PageRank optimization\n",
    "\n",
    "#### D.5: Scalable Link Farm Strategy\n",
    "\n",
    "To maximize X's PageRank with unlimited resources:\n",
    "\n",
    "1. **Create multiple farm pages** (Y, Z, W, ...) that link exclusively to X\n",
    "2. **Maintain X as dangling node** to prevent PageRank leakage\n",
    "3. **Scale effect**: Each additional farm page increases X's PageRank\n",
    "4. **Mathematical contribution**: Each farm page adds approximately `(1-α)/n` teleportation PageRank\n",
    "5. **Expected boost**: With `m` farm pages, X gains roughly `m × α × (1-α)/n` additional PageRank\n",
    "\n",
    "This strategy exploits the PageRank algorithm's fundamental mechanics while remaining within the mathematical framework of the original model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

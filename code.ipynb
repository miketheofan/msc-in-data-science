{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfEpHS6hJr-B"
   },
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17023,
     "status": "ok",
     "timestamp": 1744698947523,
     "user": {
      "displayName": "Marios Mantzaris",
      "userId": "03416491895175165913"
     },
     "user_tz": -180
    },
    "id": "2nlFTejCJrr7",
    "outputId": "9f4f7244-7fae-4ece-c586-4e7efe455af9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaeltheophanopoulos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Tuple, Set\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import heapq\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('brown')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnoQvXCdJwkx"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "d92pT_pLJyrZ"
   },
   "outputs": [],
   "source": [
    "# PART 1: N-GRAM LANGUAGE MODEL IMPLEMENTATION\n",
    "# ===========================================\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, n: int, min_freq: int = 10, tokenizer: str = 'nltk'):\n",
    "        \"\"\"\n",
    "        Initialize an n-gram language model.\n",
    "\n",
    "        Args:\n",
    "            n: The size of n-grams (2 for bigram, 3 for trigram)\n",
    "            min_freq: Minimum frequency to include a word in vocabulary\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.min_freq = min_freq\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if tokenizer == 'regexp':\n",
    "          self.regexp_tokenizer = RegexpTokenizer(pattern=r'\\w+|\\(|\\)|\\.|\\,')\n",
    "\n",
    "        # Main model components\n",
    "        self.vocabulary = set()  # Words in the vocabulary\n",
    "        self.word_counts = Counter()  # Counts of individual words\n",
    "        self.ngram_counts = defaultdict(Counter)  # Counts of n-grams\n",
    "        self.context_counts = defaultdict(int)  # Counts of (n-1)-grams (contexts)\n",
    "\n",
    "        # Model constants\n",
    "        self.UNK = \"<UNK>\"  # Out-of-vocabulary token\n",
    "        self.END = \"<end>\"  # End of sentence token\n",
    "\n",
    "        # Different start tokens for different n values\n",
    "        if n == 2:\n",
    "            self.START = [\"<start>\"]\n",
    "        elif n == 3:\n",
    "            self.START = [\"<start1>\", \"<start2>\"]\n",
    "        else:\n",
    "            self.START = [f\"<start{i}>\" for i in range(1, n)]\n",
    "\n",
    "        # Statistics\n",
    "        self.total_sentences = 0\n",
    "        self.total_tokens = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def custom_tokenize(self, text):\n",
    "            \"\"\"\n",
    "            Custom tokenizer using regex to find word tokens.\n",
    "\n",
    "            Args:\n",
    "                text: Input text string\n",
    "\n",
    "            Returns:\n",
    "                List of tokens\n",
    "            \"\"\"\n",
    "            return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "    def preprocess_text(self, corpus: List[str]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Preprocess raw text into tokenized sentences.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of text passages (paragraphs, documents, etc.)\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences\n",
    "        \"\"\"\n",
    "        tokenized_sentences = []\n",
    "\n",
    "        for text in corpus:\n",
    "            # First split the text into sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                # Clean the sentence\n",
    "                clean_sentence = sentence.lower().strip()\n",
    "\n",
    "                # Apply selected tokenizer\n",
    "                if self.tokenizer == \"nltk\":\n",
    "                    tokens = word_tokenize(clean_sentence)\n",
    "                elif self.tokenizer == \"custom\":\n",
    "                    tokens = self.custom_tokenize(clean_sentence)\n",
    "                elif self.tokenizer == \"regexp\":\n",
    "                    tokens = self.regexp_tokenizer.tokenize(clean_sentence)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown tokenizer: {self.tokenizer}\")\n",
    "\n",
    "                # Only add non-empty sentences\n",
    "                if tokens:\n",
    "                    tokenized_sentences.append(tokens)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def build_vocabulary(self, tokenized_sentences: List[List[str]]) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Build vocabulary from tokenized sentences based on minimum frequency.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            Set of vocabulary words\n",
    "        \"\"\"\n",
    "        # Count word occurrences\n",
    "        word_counter = Counter()\n",
    "        for sentence in tokenized_sentences:\n",
    "            word_counter.update(sentence)\n",
    "\n",
    "        # Create vocabulary with words that meet minimum frequency\n",
    "        vocabulary = {word for word, count in word_counter.items()\n",
    "                     if count >= self.min_freq}\n",
    "\n",
    "        # Always add special tokens to vocabulary\n",
    "        vocabulary.add(self.UNK)\n",
    "        vocabulary.add(self.END)\n",
    "        for token in self.START:\n",
    "            vocabulary.add(token)\n",
    "\n",
    "        return vocabulary\n",
    "\n",
    "    def replace_oov_words(self, tokenized_sentences: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Replace out-of-vocabulary words with UNK token.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences\n",
    "\n",
    "        Returns:\n",
    "            List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        processed_sentences = []\n",
    "\n",
    "        for sentence in tokenized_sentences:\n",
    "            processed_sentence = []\n",
    "            for token in sentence:\n",
    "                if token in self.vocabulary:\n",
    "                    processed_sentence.append(token)\n",
    "                else:\n",
    "                    processed_sentence.append(self.UNK)\n",
    "            processed_sentences.append(processed_sentence)\n",
    "\n",
    "        return processed_sentences\n",
    "\n",
    "    def extract_ngrams(self, tokenized_sentences: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        Extract n-grams from tokenized sentences and count their occurrences.\n",
    "\n",
    "        Args:\n",
    "            tokenized_sentences: List of tokenized sentences with OOV words replaced\n",
    "        \"\"\"\n",
    "        for sentence in tokenized_sentences:\n",
    "            # Add start and end tokens\n",
    "            augmented_sentence = self.START + sentence + [self.END]\n",
    "            self.total_tokens += len(sentence) + 1  # +1 for END token\n",
    "\n",
    "            # Count individual words (unigrams)\n",
    "            self.word_counts.update(augmented_sentence)\n",
    "\n",
    "            # Extract and count n-grams\n",
    "            for i in range(len(augmented_sentence) - self.n + 1):\n",
    "                ngram = tuple(augmented_sentence[i:i + self.n])\n",
    "                prefix = ngram[:-1]  # Context (n-1 gram)\n",
    "                word = ngram[-1]     # Word being predicted\n",
    "\n",
    "                self.ngram_counts[prefix][word] += 1\n",
    "                self.context_counts[prefix] += 1\n",
    "\n",
    "    def train(self, corpus: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        Train the n-gram language model on the provided corpus.\n",
    "\n",
    "        Args:\n",
    "            corpus: List of text passages that may contain multiple sentences\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.n}-gram model on corpus...\")\n",
    "\n",
    "        # Preprocess corpus\n",
    "        tokenized_sentences = self.preprocess_text(corpus)\n",
    "        self.total_sentences = len(tokenized_sentences)\n",
    "        print(f\"Extracted {self.total_sentences} sentences from corpus\")\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocabulary = self.build_vocabulary(tokenized_sentences)\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        print(f\"Vocabulary size: {self.vocabulary_size} words\")\n",
    "\n",
    "        # Replace OOV words\n",
    "        processed_sentences = self.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "        # Extract n-grams\n",
    "        self.extract_ngrams(processed_sentences)\n",
    "\n",
    "        print(f\"Extracted {sum(len(counts) for counts in self.ngram_counts.values())} unique {self.n}-grams\")\n",
    "        print(f\"Total tokens in corpus: {self.total_tokens}\")\n",
    "\n",
    "    def get_laplace_probability(self, word: str, context: tuple, alpha: float = 0.0001) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Laplace-smoothed probability P(word|context).\n",
    "\n",
    "        Args:\n",
    "            word: The word to calculate probability for\n",
    "            context: The preceding (n-1) words\n",
    "\n",
    "        Returns:\n",
    "            The conditional probability P(word|context)\n",
    "        \"\"\"\n",
    "        count_ngram = self.ngram_counts[context][word]\n",
    "        count_context = self.context_counts[context]\n",
    "\n",
    "        # Apply Laplace smoothing\n",
    "        probability = (count_ngram + alpha) / (count_context + alpha * self.vocabulary_size)\n",
    "\n",
    "        return probability\n",
    "\n",
    "    def get_interpolated_probability(self, word: str, context: tuple) -> float:\n",
    "      \"\"\"\n",
    "      Calculate interpolated probability combining n-gram levels.\n",
    "\n",
    "      Args:\n",
    "          word: The word to calculate probability for\n",
    "          context: The preceding (n-1) words\n",
    "\n",
    "      Returns:\n",
    "          Interpolated probability\n",
    "      \"\"\"\n",
    "      # Smoothing parameter\n",
    "      alpha = 0.0001\n",
    "\n",
    "      if self.n == 2:\n",
    "          # For bigram model: interpolate unigram and bigram\n",
    "          lambda1, lambda2 = 0.2, 0.8  # Weights (sum to 1)\n",
    "\n",
    "          # Unigram probability (with smoothing)\n",
    "          unigram_prob = (self.word_counts[word] + alpha) / (self.total_tokens + alpha * self.vocabulary_size)\n",
    "\n",
    "          # Bigram probability (with smoothing)\n",
    "          count_ngram = self.ngram_counts[context][word]\n",
    "          count_context = self.context_counts[context]\n",
    "          bigram_prob = (count_ngram + alpha) / (count_context + alpha * self.vocabulary_size)\n",
    "\n",
    "          return lambda1 * unigram_prob + lambda2 * bigram_prob\n",
    "\n",
    "      elif self.n == 3:\n",
    "          # For trigram model: interpolate unigram, bigram and trigram\n",
    "          lambda1, lambda2, lambda3 = 0.2, 0.2, 0.6  # Weights (sum to 1)\n",
    "\n",
    "          # Unigram probability\n",
    "          unigram_prob = (self.word_counts[word] + alpha) / (self.total_tokens + alpha * self.vocabulary_size)\n",
    "\n",
    "          # Bigram probability (taking just the last word of context)\n",
    "          bigram_context = (context[1],)\n",
    "          count_bigram = self.ngram_counts[bigram_context][word] if bigram_context in self.ngram_counts else 0\n",
    "          count_bigram_context = self.context_counts[bigram_context] if bigram_context in self.context_counts else 0\n",
    "          bigram_prob = (count_bigram + alpha) / (count_bigram_context + alpha * self.vocabulary_size)\n",
    "\n",
    "          # Trigram probability\n",
    "          count_trigram = self.ngram_counts[context][word]\n",
    "          count_trigram_context = self.context_counts[context]\n",
    "          trigram_prob = (count_trigram + alpha) / (count_trigram_context + alpha * self.vocabulary_size)\n",
    "\n",
    "          return lambda1 * unigram_prob + lambda2 * bigram_prob + lambda3 * trigram_prob\n",
    "\n",
    "      return self.get_laplace_probability(word, context)\n",
    "\n",
    "\n",
    "    def get_log_probability(self, word: str, context: tuple) -> float:\n",
    "      \"\"\"\n",
    "      Calculate log probability log(P(word|context)).\n",
    "\n",
    "      Args:\n",
    "          word: The word to calculate probability for\n",
    "          context: The preceding (n-1) words\n",
    "\n",
    "      Returns:\n",
    "          The log probability log(P(word|context))\n",
    "      \"\"\"\n",
    "      probability = self.get_interpolated_probability(word, context)\n",
    "      return math.log2(probability)\n",
    "\n",
    "    def get_sentence_log_probability(self, sentence: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the log probability of a sentence.\n",
    "\n",
    "        Args:\n",
    "            sentence: List of tokens in the sentence\n",
    "\n",
    "        Returns:\n",
    "            The log probability of the sentence\n",
    "        \"\"\"\n",
    "        # Replace OOV words with UNK\n",
    "        processed_sentence = [token if token in self.vocabulary else self.UNK for token in sentence]\n",
    "\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = self.START + processed_sentence + [self.END]\n",
    "\n",
    "        log_prob = 0.0\n",
    "\n",
    "        # Calculate log probability for each word given its context\n",
    "        for i in range(len(self.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - self.n + 1:i])\n",
    "\n",
    "            log_prob += self.get_log_probability(word, context)\n",
    "\n",
    "        return log_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_corpus(corpus_name='reuters', min_sentences=100000):\n",
    "    \"\"\"\n",
    "    Load and split a corpus from NLTK into train, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        corpus_name: Name of the corpus to load\n",
    "        min_sentences: Minimum number of sentences to include\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_corpus, val_corpus, test_corpus)\n",
    "    \"\"\"\n",
    "    print(f\"Loading {corpus_name} corpus...\")\n",
    "\n",
    "    if corpus_name == 'reuters':\n",
    "        from nltk.corpus import reuters\n",
    "        sentences = [\" \".join(reuters.words(fileid)) for fileid in reuters.fileids()]\n",
    "\n",
    "        # Break into actual sentences\n",
    "        all_sentences = []\n",
    "        for text in sentences:\n",
    "            all_sentences.extend(nltk.sent_tokenize(text))\n",
    "\n",
    "    elif corpus_name == 'brown':\n",
    "        from nltk.corpus import brown\n",
    "        sentences = [\" \".join(brown.words(fileid)) for fileid in brown.fileids()]\n",
    "\n",
    "        # Break into actual sentences\n",
    "        all_sentences = []\n",
    "        for text in sentences:\n",
    "            all_sentences.extend(nltk.sent_tokenize(text))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown corpus: {corpus_name}\")\n",
    "\n",
    "    # Ensure we have enough sentences\n",
    "    if len(all_sentences) < min_sentences:\n",
    "        raise ValueError(f\"Corpus {corpus_name} has only {len(all_sentences)} sentences, \"\n",
    "                         f\"which is less than the required {min_sentences}.\")\n",
    "\n",
    "    # Shuffle sentences\n",
    "    random.seed(42)\n",
    "    random.shuffle(all_sentences)\n",
    "\n",
    "    # Take a subset for faster processing if needed\n",
    "    sentences_subset = all_sentences[:min_sentences]\n",
    "\n",
    "    # Split into train, validation, and test sets (70%, 15%, 15%)\n",
    "    train_size = int(0.7 * len(sentences_subset))\n",
    "    val_size = int(0.15 * len(sentences_subset))\n",
    "\n",
    "    train_corpus = sentences_subset[:train_size]\n",
    "    val_corpus = sentences_subset[train_size:train_size + val_size]\n",
    "    test_corpus = sentences_subset[train_size + val_size:]\n",
    "\n",
    "    print(f\"Corpus split: {len(train_corpus)} train, {len(val_corpus)} validation, {len(test_corpus)} test sentences\")\n",
    "\n",
    "    return train_corpus, val_corpus, test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reuters corpus...\n",
      "Corpus split: 62999 train, 13500 validation, 13501 test sentences\n",
      "Training 2-gram model on corpus...\n",
      "Extracted 62999 sentences from corpus\n",
      "Vocabulary size: 6254 words\n",
      "Extracted 202220 unique 2-grams\n",
      "Total tokens in corpus: 1112859\n",
      "Training 3-gram model on corpus...\n",
      "Extracted 62999 sentences from corpus\n",
      "Vocabulary size: 6255 words\n",
      "Extracted 526586 unique 3-grams\n",
      "Total tokens in corpus: 1112859\n",
      "Common vocabulary size: 6253\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# Load and split corpus\n",
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='reuters', min_sentences=90000)\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10, tokenizer='regexp')\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10, tokenizer='regexp')\n",
    "trigram_model.train(train_corpus)\n",
    "\n",
    "# Ensure both models use the same vocabulary\n",
    "common_vocab = bigram_model.vocabulary.intersection(trigram_model.vocabulary)\n",
    "bigram_model.vocabulary = common_vocab\n",
    "trigram_model.vocabulary = common_vocab\n",
    "bigram_model.vocabulary_size = len(common_vocab)\n",
    "trigram_model.vocabulary_size = len(common_vocab)\n",
    "\n",
    "print(f\"Common vocabulary size: {len(common_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6tEY9p9J2Vr"
   },
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "8ZGOfJ8XJ386"
   },
   "outputs": [],
   "source": [
    "# PART 2: CROSS-ENTROPY AND PERPLEXITY EVALUATION\n",
    "# ==============================================\n",
    "def calculate_cross_entropy(model: NGramLanguageModel, test_corpus: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Calculate cross-entropy of a language model on a test corpus.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        test_corpus: List of test sentences\n",
    "\n",
    "    Returns:\n",
    "        Cross-entropy value\n",
    "    \"\"\"\n",
    "    # Preprocess and handle OOV words\n",
    "    tokenized_sentences = model.preprocess_text(test_corpus)\n",
    "    processed_sentences = model.replace_oov_words(tokenized_sentences)\n",
    "\n",
    "    total_log_prob = 0.0\n",
    "    total_words = 0  # N in cross-entropy formula (include <end> tokens, exclude <start> tokens)\n",
    "\n",
    "    for sentence in processed_sentences:\n",
    "        # Add start and end tokens\n",
    "        augmented_sentence = model.START + sentence + [model.END]\n",
    "\n",
    "        for i in range(len(model.START), len(augmented_sentence)):\n",
    "            word = augmented_sentence[i]\n",
    "            context = tuple(augmented_sentence[i - model.n + 1:i])\n",
    "\n",
    "            # Get log probability\n",
    "            log_prob = model.get_log_probability(word, context)\n",
    "\n",
    "            total_log_prob += log_prob\n",
    "            total_words += 1\n",
    "\n",
    "    cross_entropy = - (total_log_prob) / total_words\n",
    "\n",
    "    return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "U9XFeldBJ4DD"
   },
   "outputs": [],
   "source": [
    "def calculate_perplexity(cross_entropy: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate perplexity from cross-entropy.\n",
    "\n",
    "    Args:\n",
    "        cross_entropy: Cross-entropy value\n",
    "\n",
    "    Returns:\n",
    "        Perplexity value\n",
    "    \"\"\"\n",
    "    return 2 ** cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n",
      "Bigram model - Cross-entropy: 6.4029, Perplexity: 84.6183\n",
      "Trigram model - Cross-entropy: 6.6817, Perplexity: 102.6596\n",
      "\n",
      "Evaluating on test set...\n",
      "Bigram model - Cross-entropy: 6.3919, Perplexity: 83.9746\n",
      "Trigram model - Cross-entropy: 6.6857, Perplexity: 102.9403\n"
     ]
    }
   ],
   "source": [
    "# Part 2\n",
    "# Calculate cross-entropy and perplexity on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "\n",
    "bigram_ce_val = calculate_cross_entropy(bigram_model, val_corpus)\n",
    "bigram_ppl_val = calculate_perplexity(bigram_ce_val)\n",
    "\n",
    "trigram_ce_val = calculate_cross_entropy(trigram_model, val_corpus)\n",
    "trigram_ppl_val = calculate_perplexity(trigram_ce_val)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_val:.4f}, Perplexity: {bigram_ppl_val:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_val:.4f}, Perplexity: {trigram_ppl_val:.4f}\")\n",
    "\n",
    "# Calculate cross-entropy and perplexity on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "\n",
    "bigram_ce_test = calculate_cross_entropy(bigram_model, test_corpus)\n",
    "bigram_ppl_test = calculate_perplexity(bigram_ce_test)\n",
    "\n",
    "trigram_ce_test = calculate_cross_entropy(trigram_model, test_corpus)\n",
    "trigram_ppl_test = calculate_perplexity(trigram_ce_test)\n",
    "\n",
    "print(f\"Bigram model - Cross-entropy: {bigram_ce_test:.4f}, Perplexity: {bigram_ppl_test:.4f}\")\n",
    "print(f\"Trigram model - Cross-entropy: {trigram_ce_test:.4f}, Perplexity: {trigram_ppl_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjrSzAMJ7Ej"
   },
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "T_XE7epLJ4H0"
   },
   "outputs": [],
   "source": [
    "def get_next_word_greedy(model: NGramLanguageModel, context: tuple) -> str:\n",
    "    \"\"\"\n",
    "    Get the most probable next word given the context.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "\n",
    "    Returns:\n",
    "        Most probable next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Return the word with the highest probability\n",
    "    return max(candidates.items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "GPNhm9RaJ-8S"
   },
   "outputs": [],
   "source": [
    "def get_next_word_topk(model: NGramLanguageModel,\n",
    "                      context: tuple,\n",
    "                      k: int = 5,\n",
    "                      temperature: float = 1.0) -> str:\n",
    "    \"\"\"\n",
    "    Sample next word from top-k most probable words.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        context: Current context ((n-1) preceding words)\n",
    "        k: Number of top candidates to consider\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        Sampled next word\n",
    "    \"\"\"\n",
    "    # Get probabilities for all words in the vocabulary\n",
    "    candidates = {}\n",
    "\n",
    "    for word in model.vocabulary:\n",
    "        # Skip UNK token for generation\n",
    "        if word == model.UNK:\n",
    "            continue\n",
    "\n",
    "        prob = model.get_laplace_probability(word, context)\n",
    "        candidates[word] = prob\n",
    "\n",
    "    # Get top-k candidates\n",
    "    top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = np.power(probs, 1.0 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "    else:\n",
    "        probs = np.array([prob for _, prob in top_candidates])\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    words = [word for word, _ in top_candidates]\n",
    "    next_word = np.random.choice(words, p=probs)\n",
    "\n",
    "    return next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "s2cTq7csKA2b"
   },
   "outputs": [],
   "source": [
    "def beam_search(model: NGramLanguageModel,\n",
    "               prompt: List[str],\n",
    "               beam_width: int = 5,\n",
    "               max_length: int = 20) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Beam search for text generation.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        beam_width: Beam width\n",
    "        max_length: Maximum length of the generated sequence\n",
    "\n",
    "    Returns:\n",
    "        List of generated sequences (beams)\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize beams with start tokens + prompt\n",
    "    initial_sequence = model.START + processed_prompt\n",
    "    beams = [(initial_sequence, 0.0)]  # (sequence, log_prob)\n",
    "\n",
    "    # Generate for max_length steps\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # Expand each beam\n",
    "        for sequence, score in beams:\n",
    "            # If the sequence ended, keep it as is\n",
    "            if sequence[-1] == model.END:\n",
    "                new_beams.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            # Get context\n",
    "            context = tuple(sequence[-(model.n - 1):])\n",
    "\n",
    "            # Calculate probabilities for all possible next words\n",
    "            candidates = {}\n",
    "            for word in model.vocabulary:\n",
    "                # Skip UNK token for generation\n",
    "                if word == model.UNK:\n",
    "                    continue\n",
    "\n",
    "                log_prob = model.get_log_probability(word, context)\n",
    "                candidates[word] = log_prob\n",
    "\n",
    "            # Get top candidates\n",
    "            top_candidates = sorted(candidates.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "            # Create new beams with expanded sequences\n",
    "            for word, log_prob in top_candidates:\n",
    "                new_sequence = sequence + [word]\n",
    "                new_score = score + log_prob\n",
    "                new_beams.append((new_sequence, new_score))\n",
    "\n",
    "        # Select top beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "\n",
    "        # Check if all beams have ended\n",
    "        if all(sequence[-1] == model.END for sequence, _ in beams):\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated parts (excluding start tokens and prompt)\n",
    "    start_len = len(model.START) + len(processed_prompt)\n",
    "    return [sequence[start_len:] for sequence, _ in beams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "S_iamUVrJ4Fh"
   },
   "outputs": [],
   "source": [
    "def generate_text(model: NGramLanguageModel,\n",
    "                 prompt: List[str],\n",
    "                 max_length: int = 20,\n",
    "                 method: str = \"greedy\",\n",
    "                 top_k: int = 5,\n",
    "                 temperature: float = 1.0) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate text continuation based on the prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model\n",
    "        prompt: Initial words to continue from\n",
    "        max_length: Maximum length of the generated sequence\n",
    "        method: Generation method - \"greedy\", \"topk\", or \"nucleus\"\n",
    "        top_k: Number of top candidates to consider for sampling\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        List of words completing the prompt\n",
    "    \"\"\"\n",
    "    # Process the prompt\n",
    "    processed_prompt = [word if word in model.vocabulary else model.UNK for word in prompt]\n",
    "\n",
    "    # Initialize with start tokens + prompt\n",
    "    generated_text = model.START + processed_prompt\n",
    "\n",
    "    # Generate text until we reach max_length or end token\n",
    "    for _ in range(max_length):\n",
    "        # Get the most recent (n-1) words as context\n",
    "        context = tuple(generated_text[-(model.n - 1):])\n",
    "\n",
    "        # Get next word based on the specified method\n",
    "        if method == \"greedy\":\n",
    "            next_word = get_next_word_greedy(model, context)\n",
    "        elif method == \"topk\":\n",
    "            next_word = get_next_word_topk(model, context, top_k, temperature)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown generation method: {method}\")\n",
    "\n",
    "        # Add the generated word to the sequence\n",
    "        generated_text.append(next_word)\n",
    "\n",
    "        # Stop if we generated the end token\n",
    "        if next_word == model.END:\n",
    "            break\n",
    "\n",
    "    # Return only the newly generated part (excluding start tokens and prompt)\n",
    "    return generated_text[len(model.START) + len(processed_prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating text completions:\n",
      "\n",
      "Bigram model completions:\n",
      "[Greedy] I would like to commend the company said .\n",
      "[Top-K] I would like to commend the u .\n",
      "[Beam] I would like to commend the u .\n",
      "\n",
      "[Greedy] The president of the company said .\n",
      "[Top-K] The president of the u .\n",
      "[Beam] The president of the u .\n",
      "\n",
      "[Greedy] According to recent years .\n",
      "[Top-K] According to recent months , 000 dlrs , the u .\n",
      "[Beam] According to recent years .\n",
      "\n",
      "[Greedy] In the last few months of the company said .\n",
      "[Top-K] In the last few years and the u .\n",
      "[Beam] In the last few years .\n",
      "\n",
      "[Greedy] Experts say that the company said .\n",
      "[Top-K] Experts say that the company said .\n",
      "[Beam] Experts say that the u .\n",
      "\n",
      "\n",
      "Trigram model completions:\n",
      "[Greedy] I would like to commend the company said .\n",
      "[Top-K] I would like to commend the market s needs , the company said the government to reduce the federal reserve is expected to rise to 1\n",
      "[Beam] I would like to commend the u .\n",
      "\n",
      "[Greedy] The president of the company said .\n",
      "[Top-K] The president of the total outstanding .\n",
      "[Beam] The president of .\n",
      "\n",
      "[Greedy] According to recent corporate developments such as the dollar , the company said .\n",
      "[Top-K] According to recent agreements on wages and bonuses for work today after a 1 .\n",
      "[Beam] According to recent years .\n",
      "\n",
      "[Greedy] In the last few weeks , he said .\n",
      "[Top-K] In the last few weeks .\n",
      "[Beam] In the last few weeks .\n",
      "\n",
      "[Greedy] Experts say that what has happened in manufacturing industries , the company said .\n",
      "[Top-K] Experts say that in the u .\n",
      "[Beam] Experts say that what has happened ( to the u .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 3\n",
    "# Generate text completions\n",
    "print(\"\\nGenerating text completions:\")\n",
    "\n",
    "prompts = [\n",
    "\"I would like to commend the\",\n",
    "\"The president of\",\n",
    "\"According to recent\",\n",
    "\"In the last few\",\n",
    "\"Experts say that\"\n",
    "]\n",
    "\n",
    "print(\"\\nBigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  if bigram_model.tokenizer == \"nltk\":\n",
    "    prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "  elif bigram_model.tokenizer == \"custom\":\n",
    "    prompt_tokens = bigram_model.custom_tokenize(prompt)\n",
    "  elif bigram_model.tokenizer==\"regexp\":\n",
    "    prompt_tokens = bigram_model.regexp_tokenizer.tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(bigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != bigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(bigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != bigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(bigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != bigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()\n",
    "\n",
    "print(\"\\nTrigram model completions:\")\n",
    "for prompt in prompts:\n",
    "  if trigram_model.tokenizer == \"nltk\":\n",
    "      prompt_tokens = nltk.word_tokenize(prompt.lower())\n",
    "  elif trigram_model.tokenizer == \"custom\":\n",
    "      prompt_tokens = bigram_model.custom_tokenize(prompt)\n",
    "  elif trigram_model.tokenizer==\"regexp\":\n",
    "    prompt_tokens = trigram_model.regexp_tokenizer.tokenize(prompt.lower())\n",
    "\n",
    "  # Generate with greedy decoding\n",
    "  completion_greedy = generate_text(trigram_model, prompt_tokens, method=\"greedy\")\n",
    "  completion_text_greedy = prompt + \" \" + \" \".join([w for w in completion_greedy if w != trigram_model.END])\n",
    "  print(f\"[Greedy] {completion_text_greedy}\")\n",
    "\n",
    "  # Generate with top-k sampling\n",
    "  completion_topk = generate_text(trigram_model, prompt_tokens, method=\"topk\", top_k=5, temperature=0.7)\n",
    "  completion_text_topk = prompt + \" \" + \" \".join([w for w in completion_topk if w != trigram_model.END])\n",
    "  print(f\"[Top-K] {completion_text_topk}\")\n",
    "\n",
    "  # Generate with beam search\n",
    "  beam_completions = beam_search(trigram_model, prompt_tokens, beam_width=3)\n",
    "  top_beam = beam_completions[0]\n",
    "  completion_text_beam = prompt + \" \" + \" \".join([w for w in top_beam if w != trigram_model.END])\n",
    "  print(f\"[Beam] {completion_text_beam}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Context- Aware Spelling Correction using Beam Search_\n",
    "\n",
    "The goal is to implement a context-aware spelling corrector for noisy sentences.\n",
    "The corrector should:\n",
    "  * Use an **N-gram Language Model** to evaluate how natural candidate corrections are given the context.\n",
    "  * Use an **error model** based on **edit distance** to penalize corrections that are far from the original noisy token.\n",
    "  * Use **beam search** to explore multiple possible corrections at each step, keeping only the most promising sequences.\n",
    "  * Implement an option to **proserve tokens already in the vocabulary** (`skip_oov=True`), without generating unecessary candidates.\n",
    "  * Allow **verbose control**:\n",
    "    - If `verbose=True`, log detailed steps per token (candidates, scores, best selections).\n",
    "    - If `verbose=False`, do not show any logging, return only the corrected tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly our Corrector class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAwareSpellingCorrector:\n",
    "    \"\"\"\n",
    "    A context-aware spelling corrector using beam search,\n",
    "    organized as a class without instance variables.\n",
    "    \"\"\"\n",
    "\n",
    "    def log_prob(self, p: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute log-probability with safe handling for zero.\n",
    "        \"\"\"\n",
    "        return math.log(p) if p > 0 else float('-inf')\n",
    "\n",
    "    def softmax(self, scores):\n",
    "        \"\"\"\n",
    "        Compute softmax over a list of scores.\n",
    "        \"\"\"\n",
    "        exp_scores = [math.exp(s - max(scores)) for s in scores]\n",
    "        total = sum(exp_scores)\n",
    "        return [e / total for e in exp_scores]\n",
    "\n",
    "    def calculate_lm_score(self, candidate: str, context: Tuple[str, ...], model) -> float:\n",
    "        \"\"\"\n",
    "        Get the language model log-probability of a candidate given context.\n",
    "        \"\"\"\n",
    "        return model.get_log_probability(candidate, context)\n",
    "\n",
    "    def calculate_error_score(self, noisy_token: str, candidate: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute the log-probability of a candidate based on its edit distance.\n",
    "        \"\"\"\n",
    "        edit_dist = nltk.edit_distance(noisy_token, candidate)\n",
    "        return self.log_prob(1 / (edit_dist + 1))\n",
    "\n",
    "    def combine_scores(self, lm_score: float, error_score: float, lambda_lm: float = 0.8, lambda_err: float = 0.2) -> float:\n",
    "        \"\"\"\n",
    "        Combine language model and error model scores using weighted sum.\n",
    "        \"\"\"\n",
    "        return lambda_lm * lm_score + lambda_err * error_score\n",
    "\n",
    "    def generate_candidates(\n",
    "        self,\n",
    "        noisy_token: str,\n",
    "        vocabulary: Set[str],\n",
    "        max_edit_distance: int = 2,\n",
    "        skip_oov: bool = True\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate candidate corrections within a max edit distance.\n",
    "        \"\"\"\n",
    "        if skip_oov and noisy_token in vocabulary:\n",
    "            return [noisy_token]\n",
    "\n",
    "        candidates = []\n",
    "        for word in vocabulary:\n",
    "            if word == \"<UNK>\":\n",
    "                continue\n",
    "            if nltk.edit_distance(noisy_token, word) <= max_edit_distance:\n",
    "                candidates.append(word)\n",
    "\n",
    "        return candidates or [noisy_token]\n",
    "\n",
    "    def beam_search_step(\n",
    "        self,\n",
    "        beams: List[Tuple[List[str], float]],\n",
    "        noisy_token: str,\n",
    "        model,\n",
    "        vocabulary: Set[str],\n",
    "        beam_width: int = 5,\n",
    "        lambda_lm: float = 0.8,\n",
    "        lambda_err: float = 0.2,\n",
    "        max_edit_distance: int = 2,\n",
    "        skip_oov: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> List[Tuple[List[str], float]]:\n",
    "        \"\"\"\n",
    "        Expand beam sequences with possible corrections for the next token.\n",
    "        \"\"\"\n",
    "        new_beams = []\n",
    "        candidate_info = []\n",
    "\n",
    "        for sequence, score in beams:\n",
    "            context = tuple(sequence[-(model.n - 1):])\n",
    "            candidates = self.generate_candidates(noisy_token, vocabulary, max_edit_distance, skip_oov)\n",
    "\n",
    "            for candidate in candidates:\n",
    "                lm_score = self.calculate_lm_score(candidate, context, model)\n",
    "                err_score = self.calculate_error_score(noisy_token, candidate)\n",
    "                total_score = self.combine_scores(lm_score, err_score, lambda_lm, lambda_err)\n",
    "\n",
    "                candidate_info.append((candidate, lm_score, err_score, total_score))\n",
    "\n",
    "                new_sequence = sequence + [candidate]\n",
    "                new_score = score + total_score\n",
    "                new_beams.append((new_sequence, new_score))\n",
    "\n",
    "        if not new_beams:\n",
    "            if verbose:\n",
    "                print(f\"Warning: No valid candidates for '{noisy_token}'. Using fallback.\")\n",
    "            for sequence, score in beams:\n",
    "                new_sequence = sequence + [noisy_token]\n",
    "                new_beams.append((new_sequence, score))\n",
    "\n",
    "        top_beams = heapq.nlargest(beam_width, new_beams, key=lambda x: x[1])\n",
    "        top_tokens = [beam[0][-1] for beam in top_beams]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nToken: '{noisy_token}' | Context: {context}\")\n",
    "            print(\"Top candidates:\")\n",
    "            print(f\"{'Candidate':<15} {'LM Score':>10} {'Error Score':>12} {'Combined':>12}\")\n",
    "            print(\"-\" * 55)\n",
    "\n",
    "            for token in top_tokens:\n",
    "                for cand, lm, err, combined in candidate_info:\n",
    "                    if cand == token:\n",
    "                        print(f\"{cand:<15} {lm:>+10.4f} {err:>+12.4f} {combined:>+12.4f}\")\n",
    "                        break\n",
    "\n",
    "            print(f\"Best candidate selected: {top_tokens[0]}\")\n",
    "            print(\"-\" * 55)\n",
    "\n",
    "        return top_beams\n",
    "\n",
    "    def correct(\n",
    "        self,\n",
    "        model,\n",
    "        noisy_sentence: List[str],\n",
    "        beam_width: int = 5,\n",
    "        lambda_lm: float = 0.8,\n",
    "        lambda_err: float = 0.2,\n",
    "        max_edit_distance: int = 2,\n",
    "        skip_oov: bool = True,\n",
    "        verbose: bool = True\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Perform context-aware spelling correction on a noisy sentence using beam search.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\nStarting correction for sentence: {' '.join(noisy_sentence)}\\n\")\n",
    "\n",
    "        beams = [(model.START, 0.0)]\n",
    "\n",
    "        for noisy_token in noisy_sentence:\n",
    "            if skip_oov and noisy_token in model.vocabulary:\n",
    "                if verbose:\n",
    "                    print(f\"\\nToken: '{noisy_token}' (in vocabulary, skipping correction)\")\n",
    "                new_beams = []\n",
    "                for sequence, score in beams:\n",
    "                    new_sequence = sequence + [noisy_token]\n",
    "                    new_beams.append((new_sequence, score))\n",
    "                beams = new_beams\n",
    "                continue\n",
    "\n",
    "            beams = self.beam_search_step(\n",
    "                beams, noisy_token, model, model.vocabulary,\n",
    "                beam_width, lambda_lm, lambda_err, max_edit_distance, skip_oov, verbose\n",
    "            )\n",
    "\n",
    "        best_sequence = max(beams, key=lambda x: x[1])[0]\n",
    "        corrected = best_sequence[len(model.START):]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFinal corrected sentence: {' '.join(corrected)}\")\n",
    "\n",
    "        return corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize our bigram and trigram models using the brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading brown corpus...\n",
      "Corpus split: 35000 train, 7500 validation, 7500 test sentences\n",
      "Training 2-gram model on corpus...\n",
      "Extracted 35000 sentences from corpus\n",
      "Vocabulary size: 6091 words\n",
      "Extracted 186764 unique 2-grams\n",
      "Total tokens in corpus: 760914\n",
      "Training 3-gram model on corpus...\n",
      "Extracted 35000 sentences from corpus\n",
      "Vocabulary size: 6092 words\n"
     ]
    }
   ],
   "source": [
    "train_corpus, val_corpus, test_corpus = load_and_split_corpus(corpus_name='brown', min_sentences=50000)\n",
    "\n",
    "# Initialize and train models\n",
    "bigram_model = NGramLanguageModel(n=2, min_freq=10)\n",
    "bigram_model.train(train_corpus)\n",
    "\n",
    "trigram_model = NGramLanguageModel(n=3, min_freq=10)\n",
    "trigram_model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we test our `context aware spelling corrector` for both models using some random test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting correction for sentence: let us sey we are freends\n",
      "\n",
      "\n",
      "Token: 'let' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'us' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sey' | Context: ('us',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "by                 -6.5532      -1.0986      -4.3713\n",
      "say                -7.3942      -0.6931      -4.7138\n",
      "see                -7.9548      -0.6931      -5.0501\n",
      "so                 -8.7746      -1.0986      -5.7042\n",
      "may                -8.8423      -1.0986      -5.7448\n",
      "Best candidate selected: by\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'we' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'are' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'freends' | Context: ('are',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "Best candidate selected: freed\n",
      "-------------------------------------------------------\n",
      "\n",
      "Final corrected sentence: let us by we are freed\n",
      "\n",
      "\n",
      "Starting correction for sentence: in consequencaae of her sistero 's marriange , been moistress of hois house from a vry early period\n",
      "\n",
      "\n",
      "Token: 'in' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'consequencaae' | Context: ('in',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "consequence       -12.9451      -1.0986      -8.2065\n",
      "Best candidate selected: consequence\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'her' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sistero' | Context: ('her',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "sister            -11.1753      -0.6931      -6.9824\n",
      "sitter            -17.5991      -1.0986     -10.9989\n",
      "Best candidate selected: sister\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ''s' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'marriange' | Context: (\"'s\",)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "marriage          -16.0498      -0.6931      -9.9071\n",
      "marriages         -17.8516      -1.0986     -11.1504\n",
      "marriage          -16.0498      -0.6931      -9.9071\n",
      "marriages         -17.8516      -1.0986     -11.1504\n",
      "Best candidate selected: marriage\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'been' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'moistress' | Context: ('been',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "Best candidate selected: moistress\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'hois' | Context: ('of',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "his                -5.6784      -0.6931      -3.6843\n",
      "this               -6.3319      -1.0986      -4.2386\n",
      "his                -5.6784      -0.6931      -3.6843\n",
      "this               -6.3319      -1.0986      -4.2386\n",
      "him                -8.7774      -1.0986      -5.7059\n",
      "Best candidate selected: his\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'house' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'from' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'vry' | Context: ('a',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "very               -7.7882      -0.6931      -4.9502\n",
      "very               -7.7882      -0.6931      -4.9502\n",
      "day                -8.6101      -1.0986      -5.6055\n",
      "way                -8.7868      -1.0986      -5.7115\n",
      "boy                -9.3882      -1.0986      -6.0723\n",
      "Best candidate selected: very\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'early' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'period' (in vocabulary, skipping correction)\n",
      "\n",
      "Final corrected sentence: in consequence of her sister 's marriage , been moistress of his house from a very early period\n",
      "\n",
      "\n",
      "Starting correction for sentence: Tomorrrow well bring somethiing new , so leav today as a memoory .\n",
      "\n",
      "\n",
      "Token: 'Tomorrrow' | Context: ('<start>',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "tomorrow          -14.8824      -1.0986      -9.3689\n",
      "Best candidate selected: tomorrow\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'well' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'bring' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'somethiing' | Context: ('bring',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "something         -13.7467      -0.6931      -8.5253\n",
      "Best candidate selected: something\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'new' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'so' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'leav' | Context: ('so',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "near               -9.5830      -1.0986      -6.1893\n",
      "clear             -10.5372      -1.0986      -6.7617\n",
      "clean             -10.5917      -1.0986      -6.7945\n",
      "dear              -10.5982      -1.0986      -6.7983\n",
      "year              -13.1650      -1.0986      -8.3384\n",
      "Best candidate selected: near\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'today' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'as' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'memoory' | Context: ('a',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "Best candidate selected: memory\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: '.' (in vocabulary, skipping correction)\n",
      "\n",
      "Final corrected sentence: tomorrow well bring something new , so near today as a memory .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"let us sey we are freends\",\n",
    "    \"in consequencaae of her sistero's marriange, been moistress of hois house from a vry early period\",\n",
    "    \"Tomorrrow well bring somethiing new, so leav today as a memoory.\"\n",
    "]\n",
    "\n",
    "corrector = ContextAwareSpellingCorrector()\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected = corrector.correct(\n",
    "        bigram_model,\n",
    "        nltk.word_tokenize(sentence),\n",
    "        beam_width=5,\n",
    "        lambda_lm=0.6,\n",
    "        lambda_err=0.4,\n",
    "        skip_oov=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting correction for sentence: let us sey we are freends\n",
      "\n",
      "\n",
      "Token: 'let' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'us' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sey' | Context: ('us',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "by                 -6.5532      -1.0986      -4.3713\n",
      "say                -7.3942      -0.6931      -4.7138\n",
      "see                -7.9548      -0.6931      -5.0501\n",
      "so                 -8.7746      -1.0986      -5.7042\n",
      "may                -8.8423      -1.0986      -5.7448\n",
      "Best candidate selected: by\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'we' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'are' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'freends' | Context: ('are',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "freed             -11.7268      -1.0986      -7.4755\n",
      "Best candidate selected: freed\n",
      "-------------------------------------------------------\n",
      "\n",
      "Final corrected sentence: let us by we are freed\n",
      "\n",
      "\n",
      "Starting correction for sentence: in consequencaae of her sistero 's marriange , been moistress of hois house from a vry early period\n",
      "\n",
      "\n",
      "Token: 'in' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'consequencaae' | Context: ('in',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "consequence       -12.9451      -1.0986      -8.2065\n",
      "Best candidate selected: consequence\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'her' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'sistero' | Context: ('her',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "sister            -11.1753      -0.6931      -6.9824\n",
      "sitter            -17.5991      -1.0986     -10.9989\n",
      "Best candidate selected: sister\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ''s' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'marriange' | Context: (\"'s\",)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "marriage          -16.0498      -0.6931      -9.9071\n",
      "marriages         -17.8516      -1.0986     -11.1504\n",
      "marriage          -16.0498      -0.6931      -9.9071\n",
      "marriages         -17.8516      -1.0986     -11.1504\n",
      "Best candidate selected: marriage\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'been' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'moistress' | Context: ('been',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "moistress         -24.1564      +0.0000     -14.4938\n",
      "Best candidate selected: moistress\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'of' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'hois' | Context: ('of',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "his                -5.6784      -0.6931      -3.6843\n",
      "this               -6.3319      -1.0986      -4.2386\n",
      "his                -5.6784      -0.6931      -3.6843\n",
      "this               -6.3319      -1.0986      -4.2386\n",
      "him                -8.7774      -1.0986      -5.7059\n",
      "Best candidate selected: his\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'house' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'from' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'vry' | Context: ('a',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "very               -7.7882      -0.6931      -4.9502\n",
      "very               -7.7882      -0.6931      -4.9502\n",
      "day                -8.6101      -1.0986      -5.6055\n",
      "way                -8.7868      -1.0986      -5.7115\n",
      "boy                -9.3882      -1.0986      -6.0723\n",
      "Best candidate selected: very\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'early' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'period' (in vocabulary, skipping correction)\n",
      "\n",
      "Final corrected sentence: in consequence of her sister 's marriage , been moistress of his house from a very early period\n",
      "\n",
      "\n",
      "Starting correction for sentence: Tomorrrow well bring somethiing new , so leav today as a memoory .\n",
      "\n",
      "\n",
      "Token: 'Tomorrrow' | Context: ('<start>',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "tomorrow          -14.8824      -1.0986      -9.3689\n",
      "Best candidate selected: tomorrow\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'well' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'bring' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'somethiing' | Context: ('bring',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "something         -13.7467      -0.6931      -8.5253\n",
      "Best candidate selected: something\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'new' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: ',' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'so' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'leav' | Context: ('so',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "near               -9.5830      -1.0986      -6.1893\n",
      "clear             -10.5372      -1.0986      -6.7617\n",
      "clean             -10.5917      -1.0986      -6.7945\n",
      "dear              -10.5982      -1.0986      -6.7983\n",
      "year              -13.1650      -1.0986      -8.3384\n",
      "Best candidate selected: near\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: 'today' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'as' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'a' (in vocabulary, skipping correction)\n",
      "\n",
      "Token: 'memoory' | Context: ('a',)\n",
      "Top candidates:\n",
      "Candidate         LM Score  Error Score     Combined\n",
      "-------------------------------------------------------\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "memory            -16.1863      -0.6931      -9.9890\n",
      "Best candidate selected: memory\n",
      "-------------------------------------------------------\n",
      "\n",
      "Token: '.' (in vocabulary, skipping correction)\n",
      "\n",
      "Final corrected sentence: tomorrow well bring something new , so near today as a memory .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"let us sey we are freends\",\n",
    "    \"in consequencaae of her sistero's marriange, been moistress of hois house from a vry early period\",\n",
    "    \"Tomorrrow well bring somethiing new, so leav today as a memoory.\"\n",
    "]\n",
    "\n",
    "corrector = ContextAwareSpellingCorrector()\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected = corrector.correct(\n",
    "        bigram_model,\n",
    "        nltk.word_tokenize(sentence),\n",
    "        beam_width=5,\n",
    "        lambda_lm=0.6,\n",
    "        lambda_err=0.4,\n",
    "        skip_oov=True\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Observations_\n",
    "\n",
    "1. **Correction Accuracy**\n",
    "   * The **trigram model** generally selects better context-aware corrections compared to the **bigram model**.\n",
    "   * In the bigram model, some corrections make sense locally but fail to match the overall sentence meaning.\n",
    "   * With the trigram model, corrections like \"sey\" → \"see\" are more appropriate, because the model can consider two preceding words instead of only one, leading to more grammatically and semantically correct outputs.\n",
    "\n",
    "2. **Role of Context**\n",
    "   * In the bigram model, only the immediate previous word is available to predict the next word. This can cause the model to pick a word that fits the local pair but not the broader sentence.\n",
    "   * The trigram model, by using two preceding words, captures a richer context, helping it disambiguate between candidates that might otherwise look equally likely based on only one previous word.\n",
    "\n",
    "3. **Candidate Scoring**\n",
    "   * When examining the top candidates:\n",
    "     * The correct words often have better **combined scores** (language model + error model) in the trigram case.\n",
    "     * Even if multiple candidates have close edit distances (error model scores), the **language model score** can now differentiate better because of the stronger context window.\n",
    "   * This difference highlights how using a richer n-gram context improves the model's ability to rank correct candidates higher.\n",
    "\n",
    "4. **Stability Across Runs**\n",
    "   * Even though specific score values (LM, error, combined) can change slightly across different training runs or random seeds, the overall behavior remains consistent:\n",
    "     * The trigram model is more **stable** in choosing the most logical correction.\n",
    "     * The bigram model shows more **variability** and occasional incorrect corrections.\n",
    "\n",
    "5. **Conclusion**\n",
    "   * Moving from a bigram to a trigram model significantly improves the spelling correction performance by leveraging additional context, allowing the system to make more informed and globally consistent decisions about candidate words. The improvement is mainly due to better language model probabilities rather than changes in edit distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Artificial Test Dataset Creation_\n",
    "\n",
    "In this section we implement a class that generates an **artificially corrupted version** of the test corpus. This is necessary in order to stimulate real-world noisy inputs and evaluate the performance of the context-aware spelling corrector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly declare the class we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialTestDataset:\n",
    "    def __init__(self, sentences, error_prob=0.05, seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the artificial test dataset generator.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): List of clean sentences to corrupt.\n",
    "            error_prob (float): Probability of replacing each non-space character.\n",
    "            seed (int, optional): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.sentences = sentences\n",
    "        self.error_prob = error_prob\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        # Character set for random replacements (excluding space)\n",
    "        self.chars = list(string.ascii_letters + string.digits + string.punctuation)\n",
    "\n",
    "    def _corrupt_char(self, c):\n",
    "        # Do not corrupt whitespace; apply corruption with given probability\n",
    "        if c.isspace() or random.random() > self.error_prob:\n",
    "            return c\n",
    "        # Choose a random replacement different from the original\n",
    "        replacement = random.choice(self.chars)\n",
    "        while replacement == c:\n",
    "            replacement = random.choice(self.chars)\n",
    "        return replacement\n",
    "\n",
    "    def generate(self):\n",
    "        \"\"\"\n",
    "        Generate the corrupted dataset.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: Corrupted sentences.\n",
    "        \"\"\"\n",
    "        corrupted = []\n",
    "        for sentence in self.sentences:\n",
    "            corrupted_sentence = ''.join(self._corrupt_char(c) for c in sentence)\n",
    "            corrupted.append(corrupted_sentence)\n",
    "        return corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we test our generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing original vs. corrupted for first 5 sentences:\n",
      "============================================================\n",
      "Original:  He heard the patient voice calling .\n",
      "Corrupted: HJ heard tle pztient voice calling 6\n",
      "------------------------------------------------------------\n",
      "Original:  I was desperate to hold him , to give him whatever in this world he wanted or needed , and to keep him from the clutches of Lucille Warren .\n",
      "Corrupted: I was despeDate to hold him , to give him whatever in this world he wanted or needed , and to kee} him from the clutPhes of Lucille Warren .\n",
      "------------------------------------------------------------\n",
      "Original:  Maximum length Oversized monsters are never brought home either alive or preserved , and field measurements are obviously open to doubt because of the universal tendency to exaggerate dimensions .\n",
      "Corrupted: Maximum lengt) Oversized monsters are never brought home either alive or preserved , and field measurements are obviously open to doubt because of )he universal tendency qo exagjeraVe dimensions .\n",
      "------------------------------------------------------------\n",
      "Original:  ( D ) The Secretary of the Treasury , upon the concurrence of the Secretary of State , is authorized and directed , out of the sum covered into the Yugoslav Claims Fund pursuant to subsection ( B ) of this section , after completing the payments of such funds pursuant to subsection ( C ) of this Section , to make payment of the balance of any sum remaining in such fund to the Government of the Federal People's Republic of Yugoslavia to the extent required under Article 1 ( C ) of the Yugoslav Claims Agreement of 1948 .\n",
      "Corrupted: ( D ) The SecQetary of t8e Treasyry , upon thH concurrence of 'he Secretary of State , is authorized and directed , ou8 of the sum covered into the Yugoslav Claims Fund pursuant to subsection , B T of this section , after completing the payments of duct funds pursuant to subsection ( C ) o' this Section , to make payment of thP balance yf any sum remaining in such fund to the Govern'egt of the Federal People's Republic of Yugoslavia to 5he Kxtent required under Article 1 ( 3 ) oD the Yugowlav Claims Agreement of 1948 .\n",
      "------------------------------------------------------------\n",
      "Original:  `` Those sweet girls ?\n",
      "Corrupted: `` Those sweet girls ?\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Now, with test_corpus loaded, generate the corrupted dataset and display samples:\n",
    "generator = ArtificialTestDataset(test_corpus, error_prob=0.05, seed=42)\n",
    "corrupted_test_corpus = generator.generate()\n",
    "\n",
    "print(\"Showing original vs. corrupted for first 5 sentences:\")\n",
    "print(\"=\" * 60)\n",
    "for orig, corrupt in zip(test_corpus[:5], corrupted_test_corpus[:5]):\n",
    "    print(f\"Original:  {orig}\")\n",
    "    print(f\"Corrupted: {corrupt}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6\n",
    "Evaluation of the context-aware spelling corrector in terms of Word Error Rate (WER) and character Error Rate (CER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Character Error Rate (CER)** and **Word Error Rate (WER)** are metrics that measure the performance of the context aware spelling corrector by calculating the rate of erroneous characters produced by the system compared to the ground truth and its accuracy at the word level by measuring the proportion of incorrectly recognized words relative to the reference text.\n",
    "* Both are derived from the Levenshtein (edit) distance with values typically ranging from 0 to 1, where 0 indicates perfect alignment of the system output to the ground truth anf 1 indicates total dissimilarity between the compared pieces of text. If the score is larger than 1 we assume that the prediction is worse than a complete mismatch, with more actions required (deletion, insertion, substitution) than reference words.\n",
    "* WER is defined from the Levenshtein distance normalised by the sentence length: $$WER=\\frac{S+D+I}{N}$$ where\n",
    "  - S: number of substitutions\n",
    "  - D: number of deletions\n",
    "  - I: number of insertions\n",
    "  - N: number of words in reference text\n",
    "* CER is defined from the Levenshtein distance normalised by the sentence length: $$CER=\\frac{S+D+I}{n}$$ where\n",
    "  - n: number of characters in reference text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaeltheophanopoulos/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install jiwer\n",
    "import evaluate\n",
    "\n",
    "wer_metric = evaluate.load(\"wer\")  # Load WER metric\n",
    "cer_metric = evaluate.load(\"cer\")  # Load CER metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ArtificialTestDataset(test_corpus, error_prob=0.05, seed=42)\n",
    "corrupted_test_corpus = generator.generate()\n",
    "\n",
    "reference = test_corpus[:100] # Ground truth (list)\n",
    "hypothesis = corrupted_test_corpus # Noisy sentences (list)\n",
    "\n",
    "reference_sentences_tokenised = []\n",
    "corrupted_sentences_tokenised = [] \n",
    "corrected_sentences_tokenised = []\n",
    "\n",
    "corrector = ContextAwareSpellingCorrector()\n",
    "\n",
    "for sentence in hypothesis[:100]:\n",
    "    test_tokens = nltk.word_tokenize(sentence)\n",
    "    corrupted_sentences_tokenised.append(sentence)\n",
    "    corrected_sentences_tokenised.append(\" \".join(\n",
    "        corrector.correct(bigram_model, \n",
    "            test_tokens, \n",
    "            beam_width=5, \n",
    "            lambda_lm=0.8, \n",
    "            lambda_err=0.2,\n",
    "            skip_oov=True,\n",
    "            verbose=False)))\n",
    "\n",
    "for sentence in reference: \n",
    "    reference_sentences_tokenised.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "test_data = list(zip(reference_sentences_tokenised, corrupted_sentences_tokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.3263\n",
      "CER: 0.0967\n"
     ]
    }
   ],
   "source": [
    "wer_score = wer_metric.compute(references=reference, predictions=corrected_sentences_tokenised)\n",
    "cer_score = cer_metric.compute(references=reference, predictions=corrected_sentences_tokenised)\n",
    "\n",
    "print(f\"WER: {wer_score:.4f}\")  # Output: WER: 0.2222 (2/9 words wrong)\n",
    "print(f\"CER: {cer_score:.4f}\")  # Output: CER: 0.0513 (2/39 chars wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ArtificialTestDataset(test_corpus, error_prob=0.05, seed=42)\n",
    "corrupted_test_corpus = generator.generate()\n",
    "\n",
    "reference = test_corpus[:100] # Ground truth (list)\n",
    "hypothesis = corrupted_test_corpus # Noisy (list)\n",
    "\n",
    "reference_sentences_tokenised = []\n",
    "corrupted_sentences_tokenised = [] \n",
    "corrected_sentences_tokenised = []\n",
    "\n",
    "corrector = ContextAwareSpellingCorrector()\n",
    "\n",
    "for sentence in hypothesis[:100]:\n",
    "    test_tokens = nltk.word_tokenize(sentence)\n",
    "    corrupted_sentences_tokenised.append(sentence)\n",
    "    corrected_sentences_tokenised.append(\" \".join(\n",
    "        corrector.correct(bigram_model, \n",
    "            test_tokens, \n",
    "            beam_width=5, \n",
    "            lambda_lm=0.8, \n",
    "            lambda_err=0.2,\n",
    "            skip_oov=True,\n",
    "            verbose=False)))\n",
    "\n",
    "for sentence in reference: \n",
    "    reference_sentences_tokenised.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "test_data = list(zip(reference_sentences_tokenised, corrupted_sentences_tokenised))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.3263\n",
      "CER: 0.0967\n"
     ]
    }
   ],
   "source": [
    "wer_score = wer_metric.compute(references=reference, predictions=corrected_sentences_tokenised)\n",
    "cer_score = cer_metric.compute(references=reference, predictions=corrected_sentences_tokenised)\n",
    "\n",
    "print(f\"WER: {wer_score:.4f}\")\n",
    "print(f\"CER: {cer_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Evaluation Class\n",
    "\n",
    "The SpellingCorrectionEvaluator class encapsulates the whole pipeline which performs the evaluation of a context-aware spelling corrector (passed as a callable object argument) on artificially corrupted test data (generator method from ArtificialTestDataset instance).\n",
    "\n",
    "Steps:\n",
    "1. Takes in a list of ground truth (clean) sentences to use as references.\n",
    "2. Initializes an ArtificialTestDataset instance to generate noisy versions of the reference sentences by introducing random errors with a given probability (error_prob).\n",
    "3. Applies a context-aware spelling corrector (e.g., beam search using a trained n-gram model) on each corrupted sentence to produce a corrected hypothesis.\n",
    "4. Computes Word Error Rate (WER) and Character Error Rate (CER) by comparing the corrected hypotheses against the original clean references.\n",
    "5. Returns the WER and CER scores as evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellingCorrectionExperiment:\n",
    "    \"\"\"\n",
    "        Evaluation of the spelling_corrector_function performance on artificially corrupted data. \n",
    "\n",
    "         Args:\n",
    "             test_corpus (list): List of ground truth (reference) sentences.\n",
    "             spelling_corrector_function (callable): Function for context-aware spelling correction.\n",
    "             language_model: Trained language model passed to the corrector.\n",
    "             error_prob (float): Probability of introducing an error in the test sentences.\n",
    "             seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 test_corpus: list[str], \n",
    "                 spelling_corrector, \n",
    "                 language_model, \n",
    "                 error_prob: float =0.05, \n",
    "                 seed:int = 42, \n",
    "                 beam_width:int = 5):\n",
    "        self.reference_sentences = test_corpus\n",
    "        self.spelling_corrector = spelling_corrector\n",
    "        self.language_model = language_model\n",
    "        self.error_prob = error_prob\n",
    "        self.seed = seed\n",
    "        self.beam_width = beam_width\n",
    "        \n",
    "        self.wer_metric = evaluate.load(\"wer\")\n",
    "        self.cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "    def run(self):\n",
    "        # First generate corrupted versions of the sentences\n",
    "        generator = ArtificialTestDataset(self.reference_sentences, error_prob=self.error_prob, seed=self.seed)\n",
    "        corrupted_sentences = generator.generate()\n",
    "\n",
    "        # Second,  tokenize reference and corrupted sentences\n",
    "        corrupted_tokenized = [nltk.word_tokenize(sentence) for sentence in corrupted_sentences]\n",
    "        \n",
    "        # Third, apply correction on the corrupted sentences and show progress bar\n",
    "        corrected_sentences = []\n",
    "        for tokens in tqdm(corrupted_tokenized, desc=\"Correcting Sentences\"):\n",
    "            corrected_tokens = self.spelling_corrector.correct(\n",
    "                self.language_model, \n",
    "                tokens, \n",
    "                beam_width=self.beam_width,  # Beam width passed here\n",
    "                lambda_lm=0.8, \n",
    "                lambda_err=0.2,\n",
    "                skip_oov=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            corrected_sentences.append(\" \".join(corrected_tokens))\n",
    "\n",
    "        # Fourth, evaluate the metrics\n",
    "        wer_score = self.wer_metric.compute(references=self.reference_sentences, predictions=corrected_sentences)\n",
    "        cer_score = self.cer_metric.compute(references=self.reference_sentences, predictions=corrected_sentences)\n",
    "\n",
    "        return wer_score, cer_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Correcting Sentences: 100%|██████████| 100/100 [08:17<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.3263\n",
      "CER: 0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "spelling_corrector = ContextAwareSpellingCorrector()\n",
    "\n",
    "# Run the evaluation experiment on a sample of 100 sentences from the test corpus using the Bigram Model \n",
    "experiment = SpellingCorrectionExperiment(test_corpus[:100], spelling_corrector, bigram_model)\n",
    "wer, cer = experiment.run()\n",
    "print(f\"WER: {wer:.4f}\")\n",
    "print(f\"CER: {cer:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Correcting Sentences: 100%|██████████| 100/100 [08:15<00:00,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.3267\n",
      "CER: 0.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation experiment on a sample of 100 sentences from the test corpus using Trigram Model\n",
    "experiment = SpellingCorrectionExperiment(test_corpus[:100], corrector, trigram_model)\n",
    "wer, cer = experiment.run()\n",
    "print(f\"WER: {wer:.4f}\")\n",
    "print(f\"CER: {cer:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
